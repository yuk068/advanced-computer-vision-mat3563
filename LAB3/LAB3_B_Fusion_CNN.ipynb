{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b4c9219-37bd-4a8b-9be6-98800f4c932b",
   "metadata": {},
   "source": [
    "## LAB3 B: Fusion 2D CNN\n",
    "\n",
    "### Proceedure:\n",
    "\n",
    "- Make `DataLoader`.\n",
    "- Build, train, evaluate, and compare early fusion vs late fusion.\n",
    "- Try with pretrained backbones CNNs, compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3e6bb9-5249-4dc0-84dd-e33d2ddefc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "conda_bin = r\"E:\\Miniconda\\envs\\acv_tf\\Library\\bin\"\n",
    "os.environ['PATH'] = conda_bin + os.pathsep + os.environ['PATH']\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412615ba-ae31-4fbe-94c9-16cc632be1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset:\n",
    "    \"\"\"Video Dataset loader for Fusion experiments (Early & Late).\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split=\"train\", num_frames=16):\n",
    "        self.split = split\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # Load .npz\n",
    "        npz_path = os.path.join(root_dir, f\"{split}.npz\")\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        self.videos = list(data[\"X\"])\n",
    "        self.labels = np.array(data[\"y\"])\n",
    "        self.ids = list(data[\"ids\"])\n",
    "\n",
    "        # Class maps\n",
    "        all_classes = sorted({vid.split(\"/\")[0] for vid in self.ids})\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(all_classes)}\n",
    "        self.idx_to_class = {i: cls for cls, i in self.class_to_idx.items()}\n",
    "\n",
    "        print(f\"✅ Loaded {split}.npz: {len(self.videos)} samples, {len(all_classes)} classes\")\n",
    "\n",
    "    def _uniform_sample(self, frames, num_samples):\n",
    "        n = len(frames)\n",
    "        if n >= num_samples:\n",
    "            idxs = np.linspace(0, n - 1, num_samples).astype(int)\n",
    "            return frames[idxs]\n",
    "        else:\n",
    "            pad_len = num_samples - n\n",
    "            pad = np.zeros((pad_len, *frames.shape[1:]), dtype=frames.dtype)\n",
    "            return np.concatenate([pad, frames], axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx, fusion=\"late\"):\n",
    "        frames = self.videos[idx].astype(np.float32) / 255.0\n",
    "        sampled = self._uniform_sample(frames, self.num_frames)\n",
    "        label = int(self.labels[idx])\n",
    "        class_name = self.idx_to_class[label]\n",
    "\n",
    "        if fusion == \"early\":\n",
    "            # Concatenate frames along channel axis: (T, H, W, 3) -> (H, W, 3*T)\n",
    "            H, W, C = sampled.shape[1:]\n",
    "            sampled = sampled.reshape(H, W, C * self.num_frames)\n",
    "\n",
    "        return sampled, label, class_name\n",
    "\n",
    "\n",
    "def make_tf_dataset(dataset: VideoDataset, batch_size=4, shuffle=True, fusion=\"late\"):\n",
    "    \"\"\"\n",
    "    Returns a tf.data.Dataset that yields (X, y) only for Keras.\n",
    "    \"\"\"\n",
    "    def gen():\n",
    "        for i in range(len(dataset)):\n",
    "            # ✅ Explicitly call __getitem__ with fusion\n",
    "            x, y, _ = dataset.__getitem__(i, fusion=fusion)\n",
    "            yield x, y\n",
    "\n",
    "    if fusion == \"early\":\n",
    "        output_shapes = ((112, 112, 3 * dataset.num_frames), ())\n",
    "    else:\n",
    "        output_shapes = ((dataset.num_frames, 112, 112, 3), ())\n",
    "\n",
    "    output_types = (tf.float32, tf.int32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_types=output_types, output_shapes=output_shapes)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataset))\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bf89a6-1eb3-4179-a68d-bc4cf67c1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded train.npz: 524 samples, 5 classes\n",
      "✅ Loaded test.npz: 132 samples, 5 classes\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"data/UCF50_npz\"\n",
    "\n",
    "train_dataset = VideoDataset(root_dir, split=\"train\", num_frames=16)\n",
    "test_dataset  = VideoDataset(root_dir, split=\"test\", num_frames=16)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_ds_early = make_tf_dataset(train_dataset, batch_size=batch_size, shuffle=True, fusion=\"early\")\n",
    "test_ds_early  = make_tf_dataset(test_dataset, batch_size=batch_size, shuffle=False, fusion=\"early\")\n",
    "\n",
    "train_ds_late = make_tf_dataset(train_dataset, batch_size=batch_size, shuffle=True, fusion=\"late\")\n",
    "test_ds_late  = make_tf_dataset(test_dataset, batch_size=batch_size, shuffle=False, fusion=\"late\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00eaaaa2-ef56-4676-9322-e43159ec1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/LAB3_B\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "num_frames = 16\n",
    "input_height = 112\n",
    "input_width = 112\n",
    "batch_size = 16\n",
    "num_classes = len(train_dataset.idx_to_class)\n",
    "epochs = 5\n",
    "\n",
    "# ---------------------------\n",
    "# Build 2D CNN Backbone\n",
    "# ---------------------------\n",
    "def build_backbone(input_shape, num_classes, first_conv_channels=16):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(first_conv_channels, (3,3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d1faba-8c1a-446c-b561-f0f1ba9645e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 112, 112, 16)      6928      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 56, 56, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 56, 56, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 56, 56, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 28, 28, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1605888   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,711,093\n",
      "Trainable params: 1,711,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16, 112, 112, 3)  0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 16, 7, 7, 128)    97440     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131,749\n",
      "Trainable params: 131,749\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
    "\n",
    "# ---------------------------\n",
    "# Early Fusion Model\n",
    "# ---------------------------\n",
    "early_input_shape = (112, 112, 3*16)\n",
    "early_model = build_backbone(early_input_shape, num_classes=len(train_dataset.idx_to_class), first_conv_channels=16)\n",
    "\n",
    "early_model.compile(\n",
    "    optimizer=optimizers.Adam(),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "early_model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# Late Fusion Model\n",
    "# ---------------------------\n",
    "num_classes = len(train_dataset.idx_to_class)\n",
    "late_input_shape = (16, 112, 112, 3)\n",
    "late_input = layers.Input(shape=late_input_shape)\n",
    "\n",
    "# Backbone without final Flatten+Dense\n",
    "backbone_for_late = build_backbone((112, 112, 3), num_classes)\n",
    "\n",
    "# Grab the last conv layer before Flatten/Dense\n",
    "feature_extractor = models.Model(\n",
    "    backbone_for_late.input,\n",
    "    backbone_for_late.layers[-4].output  # should be (H, W, C)\n",
    ")\n",
    "\n",
    "# Apply TimeDistributed to extract features from each frame\n",
    "td_features = layers.TimeDistributed(feature_extractor)(late_input)\n",
    "\n",
    "# Aggregate features across time and spatial dimensions\n",
    "# td_features shape: (B, T, H, W, C)\n",
    "# reduce_mean across time + spatial dims -> (B, C)\n",
    "aggregated = layers.Lambda(lambda x: tf.reduce_mean(x, axis=[1, 2, 3]))(td_features)\n",
    "\n",
    "# Dense layers for final classification\n",
    "x = layers.Dense(256, activation='relu')(aggregated)\n",
    "late_output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "late_model = models.Model(inputs=late_input, outputs=late_output)\n",
    "late_model.compile(\n",
    "    optimizer=optimizers.Adam(),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "late_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150dda5-3610-4bfd-9b09-6d93cc3a0a67",
   "metadata": {},
   "source": [
    "### Early vs Late Fusion 2D CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c303e597-54a3-46aa-aed2-85a8c9788df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 6s 27ms/step - loss: 1.6136 - sparse_categorical_accuracy: 0.2385 - val_loss: 1.5466 - val_sparse_categorical_accuracy: 0.3182\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 3s 17ms/step - loss: 1.4752 - sparse_categorical_accuracy: 0.3492 - val_loss: 1.4764 - val_sparse_categorical_accuracy: 0.3864\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 3s 17ms/step - loss: 1.3210 - sparse_categorical_accuracy: 0.4160 - val_loss: 1.3928 - val_sparse_categorical_accuracy: 0.3182\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 3s 17ms/step - loss: 1.2014 - sparse_categorical_accuracy: 0.4847 - val_loss: 1.5090 - val_sparse_categorical_accuracy: 0.4091\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 3s 16ms/step - loss: 1.0644 - sparse_categorical_accuracy: 0.5534 - val_loss: 1.2609 - val_sparse_categorical_accuracy: 0.4167\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 3s 17ms/step - loss: 0.9008 - sparse_categorical_accuracy: 0.6355 - val_loss: 0.8653 - val_sparse_categorical_accuracy: 0.6818\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 3s 18ms/step - loss: 0.7842 - sparse_categorical_accuracy: 0.7042 - val_loss: 0.9416 - val_sparse_categorical_accuracy: 0.6136\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 3s 16ms/step - loss: 0.7517 - sparse_categorical_accuracy: 0.7042 - val_loss: 0.7858 - val_sparse_categorical_accuracy: 0.6818\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 3s 17ms/step - loss: 0.6141 - sparse_categorical_accuracy: 0.7824 - val_loss: 0.7289 - val_sparse_categorical_accuracy: 0.7121\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 3s 17ms/step - loss: 0.6322 - sparse_categorical_accuracy: 0.7748 - val_loss: 0.7499 - val_sparse_categorical_accuracy: 0.7348\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Train & Save Early Fusion\n",
    "# ---------------------------\n",
    "history_early = early_model.fit(\n",
    "    train_ds_early,\n",
    "    validation_data=test_ds_early,\n",
    "    epochs=10\n",
    ")\n",
    "early_model.save(os.path.join(save_dir, \"early_fusion_model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1953ae-87c0-4383-918b-979c69f1bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 5s 41ms/step - loss: 1.5963 - sparse_categorical_accuracy: 0.2061 - val_loss: 1.5513 - val_sparse_categorical_accuracy: 0.2045\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 4s 37ms/step - loss: 1.3048 - sparse_categorical_accuracy: 0.4141 - val_loss: 1.0164 - val_sparse_categorical_accuracy: 0.6288\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 4s 37ms/step - loss: 0.9588 - sparse_categorical_accuracy: 0.6050 - val_loss: 1.0161 - val_sparse_categorical_accuracy: 0.5076\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 4s 37ms/step - loss: 0.9060 - sparse_categorical_accuracy: 0.6145 - val_loss: 0.8551 - val_sparse_categorical_accuracy: 0.6894\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 4s 38ms/step - loss: 0.8217 - sparse_categorical_accuracy: 0.6622 - val_loss: 0.7799 - val_sparse_categorical_accuracy: 0.6818\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 4s 39ms/step - loss: 0.9112 - sparse_categorical_accuracy: 0.6355 - val_loss: 0.9735 - val_sparse_categorical_accuracy: 0.5758\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 4s 39ms/step - loss: 0.7476 - sparse_categorical_accuracy: 0.7118 - val_loss: 0.6048 - val_sparse_categorical_accuracy: 0.8182\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 5s 42ms/step - loss: 0.6403 - sparse_categorical_accuracy: 0.7347 - val_loss: 0.5536 - val_sparse_categorical_accuracy: 0.8636\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 5s 47ms/step - loss: 0.5668 - sparse_categorical_accuracy: 0.7844 - val_loss: 0.5148 - val_sparse_categorical_accuracy: 0.8182\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 5s 46ms/step - loss: 0.4247 - sparse_categorical_accuracy: 0.8492 - val_loss: 0.5032 - val_sparse_categorical_accuracy: 0.7727\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Train & Save Late Fusion\n",
    "# ---------------------------\n",
    "history_late = late_model.fit(\n",
    "    train_ds_late,\n",
    "    validation_data=test_ds_late,\n",
    "    epochs=10\n",
    ")\n",
    "late_model.save(os.path.join(save_dir, \"late_fusion_model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7b06f2-3c31-484e-8ed3-a17529c28e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 35ms/step - loss: 0.7499 - sparse_categorical_accuracy: 0.7348\n",
      "17/17 [==============================] - 1s 34ms/step - loss: 0.5032 - sparse_categorical_accuracy: 0.7727\n",
      "Early Fusion Test Loss: 0.7499, Accuracy: 0.7348\n",
      "Late Fusion  Test Loss: 0.5032, Accuracy: 0.7727\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Evaluate & Compare\n",
    "# ---------------------------\n",
    "early_eval = early_model.evaluate(test_ds_early)\n",
    "late_eval  = late_model.evaluate(test_ds_late)\n",
    "\n",
    "print(f\"Early Fusion Test Loss: {early_eval[0]:.4f}, Accuracy: {early_eval[1]:.4f}\")\n",
    "print(f\"Late Fusion  Test Loss: {late_eval[0]:.4f}, Accuracy: {late_eval[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75954a0c-0c33-4879-b4f7-78450579eac1",
   "metadata": {},
   "source": [
    "### Pretrained Backbone Test\n",
    "\n",
    "Note: Yeah I'm not switching to PyTorch just for this, probably gonna do PyTorch for the next 2 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7137cbd9-5704-4ec6-9f71-56c7bff8dbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running early fusion with resnet backbone...\n",
      "Epoch 1/10\n",
      "66/66 [==============================] - 9s 58ms/step - loss: 3.0712 - sparse_categorical_accuracy: 0.3359 - val_loss: 7.0185 - val_sparse_categorical_accuracy: 0.2576\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 5s 48ms/step - loss: 1.7527 - sparse_categorical_accuracy: 0.4408 - val_loss: 2.4793 - val_sparse_categorical_accuracy: 0.2121\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 5s 48ms/step - loss: 1.3550 - sparse_categorical_accuracy: 0.5630 - val_loss: 1.8158 - val_sparse_categorical_accuracy: 0.3788\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 5s 48ms/step - loss: 1.1481 - sparse_categorical_accuracy: 0.6011 - val_loss: 1.1249 - val_sparse_categorical_accuracy: 0.5606\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 5s 49ms/step - loss: 0.9899 - sparse_categorical_accuracy: 0.6660 - val_loss: 1.1365 - val_sparse_categorical_accuracy: 0.6061\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 5s 48ms/step - loss: 0.7765 - sparse_categorical_accuracy: 0.7691 - val_loss: 1.6780 - val_sparse_categorical_accuracy: 0.4924\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 5s 48ms/step - loss: 1.0240 - sparse_categorical_accuracy: 0.6718 - val_loss: 1.3416 - val_sparse_categorical_accuracy: 0.5455\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 5s 49ms/step - loss: 0.6117 - sparse_categorical_accuracy: 0.7996 - val_loss: 1.5285 - val_sparse_categorical_accuracy: 0.5909\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 5s 47ms/step - loss: 0.7272 - sparse_categorical_accuracy: 0.7634 - val_loss: 2.7408 - val_sparse_categorical_accuracy: 0.4394\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 5s 48ms/step - loss: 0.5809 - sparse_categorical_accuracy: 0.8321 - val_loss: 0.9551 - val_sparse_categorical_accuracy: 0.7045\n",
      "17/17 [==============================] - 1s 29ms/step - loss: 0.9551 - sparse_categorical_accuracy: 0.7045\n",
      "Running late fusion with resnet backbone...\n",
      "Epoch 1/10\n",
      "66/66 [==============================] - 33s 387ms/step - loss: 1.8388 - sparse_categorical_accuracy: 0.4141 - val_loss: 2.3048 - val_sparse_categorical_accuracy: 0.1515\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 25s 358ms/step - loss: 1.2473 - sparse_categorical_accuracy: 0.5439 - val_loss: 1.7334 - val_sparse_categorical_accuracy: 0.3712\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 25s 358ms/step - loss: 1.0917 - sparse_categorical_accuracy: 0.5992 - val_loss: 4.4125 - val_sparse_categorical_accuracy: 0.1894\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 25s 360ms/step - loss: 0.9057 - sparse_categorical_accuracy: 0.6431 - val_loss: 9.2506 - val_sparse_categorical_accuracy: 0.4773\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 25s 359ms/step - loss: 0.8606 - sparse_categorical_accuracy: 0.6775 - val_loss: 2.0894 - val_sparse_categorical_accuracy: 0.5152\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 25s 359ms/step - loss: 0.7787 - sparse_categorical_accuracy: 0.7195 - val_loss: 0.8734 - val_sparse_categorical_accuracy: 0.6742\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 25s 359ms/step - loss: 0.7196 - sparse_categorical_accuracy: 0.7462 - val_loss: 1.2456 - val_sparse_categorical_accuracy: 0.6288\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 25s 359ms/step - loss: 0.6942 - sparse_categorical_accuracy: 0.7347 - val_loss: 1.4802 - val_sparse_categorical_accuracy: 0.6742\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 25s 359ms/step - loss: 0.5712 - sparse_categorical_accuracy: 0.8130 - val_loss: 1.5312 - val_sparse_categorical_accuracy: 0.6439\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 25s 360ms/step - loss: 0.5264 - sparse_categorical_accuracy: 0.8321 - val_loss: 1.5061 - val_sparse_categorical_accuracy: 0.6061\n",
      "17/17 [==============================] - 1s 61ms/step - loss: 1.5061 - sparse_categorical_accuracy: 0.6061\n",
      "Running early fusion with efficientnet backbone...\n",
      "Epoch 1/10\n",
      "66/66 [==============================] - 9s 49ms/step - loss: 1.8973 - sparse_categorical_accuracy: 0.3302 - val_loss: 1.6300 - val_sparse_categorical_accuracy: 0.1894\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 4s 37ms/step - loss: 1.3904 - sparse_categorical_accuracy: 0.4561 - val_loss: 1.6514 - val_sparse_categorical_accuracy: 0.2273\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 4s 37ms/step - loss: 1.1374 - sparse_categorical_accuracy: 0.5630 - val_loss: 2.4111 - val_sparse_categorical_accuracy: 0.1894\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 4s 37ms/step - loss: 0.9731 - sparse_categorical_accuracy: 0.6412 - val_loss: 2.0086 - val_sparse_categorical_accuracy: 0.1515\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 4s 37ms/step - loss: 0.8536 - sparse_categorical_accuracy: 0.7061 - val_loss: 2.4352 - val_sparse_categorical_accuracy: 0.1515\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 4s 38ms/step - loss: 0.7120 - sparse_categorical_accuracy: 0.7347 - val_loss: 2.2391 - val_sparse_categorical_accuracy: 0.2348\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 4s 40ms/step - loss: 0.5306 - sparse_categorical_accuracy: 0.8244 - val_loss: 3.5284 - val_sparse_categorical_accuracy: 0.3106\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 4s 40ms/step - loss: 0.5646 - sparse_categorical_accuracy: 0.8092 - val_loss: 2.3085 - val_sparse_categorical_accuracy: 0.5455\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 4s 39ms/step - loss: 0.3785 - sparse_categorical_accuracy: 0.8740 - val_loss: 1.5414 - val_sparse_categorical_accuracy: 0.5455\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 4s 38ms/step - loss: 0.3694 - sparse_categorical_accuracy: 0.8645 - val_loss: 1.1045 - val_sparse_categorical_accuracy: 0.7273\n",
      "17/17 [==============================] - 1s 29ms/step - loss: 1.1045 - sparse_categorical_accuracy: 0.7273\n",
      "Running late fusion with efficientnet backbone...\n",
      "Epoch 1/10\n",
      "66/66 [==============================] - 25s 259ms/step - loss: 1.4949 - sparse_categorical_accuracy: 0.4160 - val_loss: 1.7524 - val_sparse_categorical_accuracy: 0.2273\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 17s 238ms/step - loss: 1.2218 - sparse_categorical_accuracy: 0.5000 - val_loss: 7.3768 - val_sparse_categorical_accuracy: 0.1515\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 17s 239ms/step - loss: 0.9605 - sparse_categorical_accuracy: 0.6603 - val_loss: 59.5344 - val_sparse_categorical_accuracy: 0.2273\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 17s 238ms/step - loss: 0.7733 - sparse_categorical_accuracy: 0.7519 - val_loss: 8.0132 - val_sparse_categorical_accuracy: 0.2121\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 17s 238ms/step - loss: 0.6886 - sparse_categorical_accuracy: 0.7576 - val_loss: 1.5596 - val_sparse_categorical_accuracy: 0.4470\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 18s 243ms/step - loss: 0.4379 - sparse_categorical_accuracy: 0.8511 - val_loss: 0.4929 - val_sparse_categorical_accuracy: 0.8939\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 18s 243ms/step - loss: 0.4539 - sparse_categorical_accuracy: 0.8626 - val_loss: 1.3615 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 17s 240ms/step - loss: 0.3367 - sparse_categorical_accuracy: 0.8969 - val_loss: 5.1079 - val_sparse_categorical_accuracy: 0.4318\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 18s 243ms/step - loss: 0.3249 - sparse_categorical_accuracy: 0.9084 - val_loss: 1.3170 - val_sparse_categorical_accuracy: 0.8485\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 18s 243ms/step - loss: 0.3103 - sparse_categorical_accuracy: 0.8989 - val_loss: 0.7243 - val_sparse_categorical_accuracy: 0.8333\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 0.7243 - sparse_categorical_accuracy: 0.8333\n",
      "\n",
      "--- Evaluation Comparison ---\n",
      "early_resnet: Loss=0.9551, Accuracy=0.7045\n",
      "late_resnet: Loss=1.5061, Accuracy=0.6061\n",
      "early_efficientnet: Loss=1.1045, Accuracy=0.7273\n",
      "late_efficientnet: Loss=0.7243, Accuracy=0.8333\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "NUM_CLASSES = len(train_dataset.idx_to_class)  # adjust as needed\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "backbones = {\n",
    "    \"resnet\": ResNet50,\n",
    "    \"efficientnet\": EfficientNetB0\n",
    "}\n",
    "\n",
    "fusion_types = [\"early\", \"late\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: build backbone\n",
    "# ---------------------------\n",
    "def build_backbone(backbone_fn, input_shape=(112,112,3), num_classes=NUM_CLASSES):\n",
    "    base = backbone_fn(\n",
    "        include_top=False, \n",
    "        input_shape=input_shape,\n",
    "        weights=None,  # training from scratch\n",
    "        pooling='avg'\n",
    "    )\n",
    "    x = layers.Dense(num_classes, activation='softmax')(base.output)\n",
    "    model = models.Model(inputs=base.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# Training & Evaluation Loop\n",
    "# ---------------------------\n",
    "for bb_name, bb_fn in backbones.items():\n",
    "    for fusion in fusion_types:\n",
    "        print(f\"Running {fusion} fusion with {bb_name} backbone...\")\n",
    "\n",
    "        if fusion == \"early\":\n",
    "            # Early fusion: stack frames along channels\n",
    "            early_input_shape = (112, 112, 3*16)\n",
    "            early_model = build_backbone(bb_fn, input_shape=early_input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "            early_model.compile(\n",
    "                optimizer=optimizers.Adam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy()]\n",
    "            )\n",
    "\n",
    "            # Train\n",
    "            history = early_model.fit(\n",
    "                train_ds_early,   # should yield (B,112,112,3*16), y\n",
    "                validation_data=test_ds_early,\n",
    "                epochs=EPOCHS\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            eval_res = early_model.evaluate(test_ds_early)\n",
    "            results[f\"{fusion}_{bb_name}\"] = eval_res\n",
    "\n",
    "        elif fusion == \"late\":\n",
    "            # Late fusion: per-frame features\n",
    "            late_input = layers.Input(shape=(16,112,112,3))\n",
    "            backbone_model = build_backbone(bb_fn, input_shape=(112,112,3), num_classes=NUM_CLASSES)\n",
    "\n",
    "            # Remove final Dense to get features\n",
    "            feature_extractor = models.Model(\n",
    "                backbone_model.input,\n",
    "                backbone_model.layers[-2].output  # shape (C,)\n",
    "            )\n",
    "\n",
    "            td_features = layers.TimeDistributed(feature_extractor)(late_input)  # (B,T,C)\n",
    "            aggregated = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(td_features)  # average over time\n",
    "\n",
    "            x = layers.Dense(256, activation='relu')(aggregated)\n",
    "            late_output = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "            late_model = models.Model(inputs=late_input, outputs=late_output)\n",
    "\n",
    "            late_model.compile(\n",
    "                optimizer=optimizers.Adam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy()]\n",
    "            )\n",
    "\n",
    "            # Train\n",
    "            history = late_model.fit(\n",
    "                train_ds_late,   # should yield (B,16,112,112,3), y\n",
    "                validation_data=test_ds_late,\n",
    "                epochs=EPOCHS\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            eval_res = late_model.evaluate(test_ds_late)\n",
    "            results[f\"{fusion}_{bb_name}\"] = eval_res\n",
    "\n",
    "# ---------------------------\n",
    "# Comparison\n",
    "# ---------------------------\n",
    "print(\"\\n--- Evaluation Comparison ---\")\n",
    "for k,v in results.items():\n",
    "    print(f\"{k}: Loss={v[0]:.4f}, Accuracy={v[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (acv_tf)",
   "language": "python",
   "name": "acv_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
