{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c62922",
   "metadata": {},
   "source": [
    "\n",
    "# 2D CNN Action Classification with Fusion (Early & Late) on UCF50 — PyTorch\n",
    "\n",
    "**Mục tiêu:** Xây dựng pipeline phân loại hành động (action classification) từ video bằng **2D CNN** với các cơ chế **Fusion** dạng *early* và *late* có thể chọn bằng cấu hình.  \n",
    "**Bộ dữ liệu:** [UCF50](https://www.crcv.ucf.edu/data/UCF50.php) — tổ chức theo thư mục `root/ClassName/*.avi`.\n",
    "\n",
    "**Bạn sẽ nhận được:**\n",
    "- Bộ **DataLoader** đọc video (`OpenCV`), lấy mẫu khung hình, tính **Optical Flow** (Farneback/TV-L1) có hỗ trợ **cache** ra đĩa.\n",
    "- Các mô hình:\n",
    "  - `Early-Channel Fusion`: ghép RGB + Flow theo **chiều kênh** (C) và **sửa conv1** để nhận nhiều kênh.\n",
    "  - `Early-Feature Fusion`: hai backbone riêng cho RGB/Flow, **nối đặc trưng** trước classifier.\n",
    "  - `Late Fusion`: hai backbone riêng, **trung bình/weighted** các **logits** (hoặc xác suất).\n",
    "- Vòng lặp huấn luyện có **Early Stopping**, **LR Scheduler**, lưu **best model**, đánh giá và **Confusion Matrix**.\n",
    "- Hàm **suy luận** trên 1 video.\n",
    "\n",
    "> **Gợi ý tài nguyên:** UCF50 tương đối nhẹ, nhưng tính Optical Flow on-the-fly vẫn tốn thời gian. Với máy yếu, giảm `flow_stack`, `num_segments`, hoặc bật **cache**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28738d6",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích (cell này):**  \n",
    "- **Mục đích:** Giới thiệu tổng quan bài toán và những thành phần bạn sẽ có trong notebook.  \n",
    "- **Đầu vào:** Không yêu cầu đầu vào.  \n",
    "- **Đầu ra:** Không có.  \n",
    "- **Kết quả kỳ vọng:** Bạn nắm được cấu trúc tổng thể notebook và các tùy chọn Fusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Tùy chọn) Cài đặt thư viện nếu môi trường thiếu.\n",
    "# Bạn có thể comment nếu đã có sẵn.\n",
    "# !pip install opencv-python tqdm\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b6c4c",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Import thư viện cần thiết; có cell cài đặt nếu thiếu.  \n",
    "- **Đầu vào:** Không.  \n",
    "- **Đầu ra:** Không.  \n",
    "- **Kết quả kỳ vọng:** Môi trường sẵn sàng với `OpenCV`, `PyTorch`, `torchvision`, `tqdm`, `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Đường dẫn & IO\n",
    "    DATA_ROOT: str = \"/kaggle/input/ucf50/UCF50\"   # Sửa theo nơi lưu dữ liệu (VD: local)\n",
    "    OUTPUT_DIR: str = \"./outputs_ucf50_fusion\"\n",
    "    LABELS_JSON: str = \"labels_map.json\"\n",
    "    FLOW_CACHE_DIR: str = \"./flow_cache\"           # Cache *.npy cho optical flow (tùy chọn)\n",
    "\n",
    "    # Dataloader\n",
    "    IMG_SIZE: int = 224\n",
    "    BATCH_SIZE: int = 16\n",
    "    NUM_WORKERS: int = 2\n",
    "    VAL_SPLIT: float = 0.2\n",
    "    SEED: int = 1337\n",
    "\n",
    "    # Lấy mẫu video\n",
    "    NUM_SEGMENTS: int = 1          # số frame mẫu (cho RGB). Two-Stream 2D thường 1 frame/clip\n",
    "    FLOW_STACK: int = 5            # số cặp (u,v) liên tiếp => 2*FLOW_STACK kênh\n",
    "    FLOW_METHOD: str = \"farneback\" # \"farneback\" | \"tvl1\"\n",
    "    FLOW_PRECOMPUTE: bool = True   # bật cache optical flow ra đĩa\n",
    "\n",
    "    # Huấn luyện\n",
    "    NUM_EPOCHS: int = 10\n",
    "    LEARNING_RATE: float = 1e-3\n",
    "    WEIGHT_DECAY: float = 1e-4\n",
    "    SCHEDULER: str = \"cosine\"      # \"cosine\" | \"step\" | \"plateau\" | \"none\"\n",
    "    STEP_SIZE: int = 5\n",
    "    GAMMA: float = 0.1\n",
    "    T_MAX: int = 10                # cho CosineAnnealingLR\n",
    "    EARLY_STOP_PATIENCE: int = 5\n",
    "\n",
    "    # Fusion\n",
    "    FUSION: str = \"late\"           # \"late\" | \"early_feature\" | \"early_channel\"\n",
    "    LATE_FUSION_WEIGHTS: tuple = (0.5, 0.5)  # (w_rgb, w_flow)\n",
    "    # Nếu early_channel: in_channels = 3 + 2*FLOW_STACK\n",
    "    # Nếu early_feature: concat features: [feat_rgb ; feat_flow]\n",
    "\n",
    "    # Lưu & Log\n",
    "    SAVE_BEST: bool = True\n",
    "    BEST_MODEL_PATH: str = \"best_model.pth\"\n",
    "    LOG_JSON: str = \"train_log.json\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Tạo thư mục xuất\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(cfg.FLOW_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Cố định seed cho tái lập\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.SEED)\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37daebce",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Khai báo toàn bộ cấu hình trong một `dataclass`.  \n",
    "- **Đầu vào:** Không. Bạn sửa trực tiếp trong cell nếu cần (đặc biệt `DATA_ROOT`).  \n",
    "- **Đầu ra:** Tạo thư mục `OUTPUT_DIR`, `FLOW_CACHE_DIR`. In ra cấu hình.  \n",
    "- **Kết quả kỳ vọng:** Thay đổi cấu hình là trung tâm: kiểu fusion, scheduler, batch size, số epoch...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scan_ucf50(root: str):\n",
    "    root = Path(root)\n",
    "    classes = sorted([p.name for p in root.iterdir() if p.is_dir()])\n",
    "    items = []\n",
    "    for ci, cname in enumerate(classes):\n",
    "        for v in (root/cname).glob(\"*.avi\"):\n",
    "            items.append({\"path\": str(v), \"label_name\": cname, \"label\": ci})\n",
    "    return classes, items\n",
    "\n",
    "classes, items = scan_ucf50(cfg.DATA_ROOT)\n",
    "num_classes = len(classes)\n",
    "print(f\"Found {len(items)} videos across {num_classes} classes.\")\n",
    "\n",
    "# Lưu nhãn\n",
    "labels_map = {i: c for i, c in enumerate(classes)}\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, cfg.LABELS_JSON), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Stratified split theo lớp\n",
    "from collections import defaultdict\n",
    "by_class = defaultdict(list)\n",
    "for it in items:\n",
    "    by_class[it[\"label\"]].append(it)\n",
    "\n",
    "train_list, val_list = [], []\n",
    "for k, vids in by_class.items():\n",
    "    n = len(vids)\n",
    "    idx = list(range(n))\n",
    "    random.shuffle(idx)\n",
    "    cut = int(n * (1.0 - cfg.VAL_SPLIT))\n",
    "    for j in idx[:cut]:\n",
    "        train_list.append(vids[j])\n",
    "    for j in idx[cut:]:\n",
    "        val_list.append(vids[j])\n",
    "\n",
    "print(f\"Train videos: {len(train_list)}, Val videos: {len(val_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364f0b9",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Dò tất cả video trong UCF50, tạo **nhãn** từ tên thư mục và **chia train/val** theo tỷ lệ trong `cfg.VAL_SPLIT`.  \n",
    "- **Đầu vào:** `DATA_ROOT` trỏ tới cấu trúc `UCF50/ClassName/*.avi`.  \n",
    "- **Đầu ra:** Danh sách `train_list`, `val_list` (mỗi phần tử gồm `path`, `label`, `label_name`), file `labels_map.json`.  \n",
    "- **Kết quả kỳ vọng:** Thông tin số lượng video mỗi tập, số lớp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_video_frames(path, target_indices, resize_hw=None):\n",
    "    '''Đọc một số frame theo chỉ số trong video. Trả về list ảnh BGR (np.uint8).'''\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    if not cap.isOpened():\n",
    "        cap.release()\n",
    "        raise RuntimeError(f\"Cannot open video: {path}\")\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for ti in target_indices:\n",
    "        idx = min(max(int(ti), 0), max(total-1, 0))\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            ok2, frame2 = cap.read()\n",
    "            if not ok2:\n",
    "                frame = np.zeros((resize_hw[0], resize_hw[1], 3), dtype=np.uint8) if resize_hw else None\n",
    "            else:\n",
    "                frame = frame2\n",
    "        if resize_hw is not None and frame is not None:\n",
    "            frame = cv2.resize(frame, (resize_hw[1], resize_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def compute_flow_pair(prev_gray, next_gray, method=\"farneback\"):\n",
    "    if method == \"farneback\":\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray,\n",
    "                                            None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        return flow  # HxWx2 (u,v)\n",
    "    elif method == \"tvl1\":\n",
    "        tvl1 = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "        flow = tvl1.calc(prev_gray, next_gray, None)\n",
    "        return flow\n",
    "    else:\n",
    "        raise ValueError(\"FLOW_METHOD must be 'farneback' or 'tvl1'\")\n",
    "\n",
    "def flow_to_uv_stack(frames_gray, flow_stack=5, method=\"farneback\"):\n",
    "    '''Tính (u,v) cho flow_stack cặp khung hình liên tiếp quanh frame trung tâm.\n",
    "       Trả về array shape (2*flow_stack, H, W), dtype float32.'''\n",
    "    T = len(frames_gray)\n",
    "    if T < flow_stack + 1:\n",
    "        while len(frames_gray) < flow_stack + 1:\n",
    "            frames_gray.append(frames_gray[-1])\n",
    "        T = len(frames_gray)\n",
    "    uv_list = []\n",
    "    for i in range(flow_stack):\n",
    "        f0 = frames_gray[i]\n",
    "        f1 = frames_gray[i+1]\n",
    "        flow = compute_flow_pair(f0, f1, method=method)  # HxWx2\n",
    "        u, v = flow[..., 0], flow[..., 1]\n",
    "        uv_list.append(u.astype(np.float32))\n",
    "        uv_list.append(v.astype(np.float32))\n",
    "    uv = np.stack(uv_list, axis=0)  # (2*flow_stack, H, W)\n",
    "    uv = np.clip(uv, -20.0, 20.0) / 20.0  # [-1,1]\n",
    "    return uv\n",
    "\n",
    "def load_or_compute_flow_stack(video_path, center_idx, resize_hw, flow_stack, method, cache_dir):\n",
    "    '''Tải từ cache hoặc tính flow quanh center_idx. Cache theo tên file.'''\n",
    "    from pathlib import Path\n",
    "    key = f\"{Path(video_path).stem}_f{center_idx}_s{flow_stack}_{method}_{resize_hw[0]}x{resize_hw[1]}.npy\"\n",
    "    cache_path = Path(cache_dir)/key\n",
    "    if cache_path.exists():\n",
    "        return np.load(str(cache_path))\n",
    "    frame_indices = [center_idx + i for i in range(flow_stack+1)]\n",
    "    frames = read_video_frames(video_path, frame_indices, resize_hw)\n",
    "    grays = [cv2.cvtColor(f, cv2.COLOR_BGR2GRAY) for f in frames]\n",
    "    uv = flow_to_uv_stack(grays, flow_stack=flow_stack, method=method)\n",
    "    np.save(str(cache_path), uv)\n",
    "    return uv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40576c42",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Định nghĩa utility đọc một số frame từ video và tính **Optical Flow** cho một **stack** cặp khung hình liên tiếp.  \n",
    "- **Đầu vào:** `video_path`, chỉ số frame trung tâm, `FLOW_STACK`, kích thước `resize_hw`, `FLOW_METHOD`.  \n",
    "- **Đầu ra:** Tensor `uv` dạng `(2*FLOW_STACK, H, W)` đã được cắt/chuẩn hóa về [-1, 1]. Có hỗ trợ **cache** dưới dạng `.npy`.  \n",
    "- **Kết quả kỳ vọng:** Tối ưu tốc độ bằng cache; khi đổi `FLOW_METHOD` hoặc kích thước/stack thì cache khác nhau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UCF50TwoStreamDataset(Dataset):\n",
    "    def __init__(self, items, img_size=224, num_segments=1, flow_stack=5,\n",
    "                 flow_method=\"farneback\", flow_cache_dir=None, mode=\"train\"):\n",
    "        self.items = items\n",
    "        self.H = img_size\n",
    "        self.W = img_size\n",
    "        self.num_segments = num_segments\n",
    "        self.flow_stack = flow_stack\n",
    "        self.flow_method = flow_method\n",
    "        self.flow_cache_dir = flow_cache_dir\n",
    "        self.mode = mode\n",
    "\n",
    "        self.rgb_train_tf = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.H, self.W)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.rgb_val_tf = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.H, self.W)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def _sample_center(self, vpath):\n",
    "        cap = cv2.VideoCapture(vpath)\n",
    "        if not cap.isOpened():\n",
    "            return 0, 1\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        if total <= 1:\n",
    "            return 0, 1\n",
    "        ci = random.randint(1, max(total-2, 1))\n",
    "        return ci, total\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.items[idx]\n",
    "        vpath = rec[\"path\"]\n",
    "        label = rec[\"label\"]\n",
    "\n",
    "        center_idx, total = self._sample_center(vpath)\n",
    "\n",
    "        rgb_frame = read_video_frames(vpath, [center_idx], resize_hw=(self.H, self.W))[0]  # BGR\n",
    "        rgb = cv2.cvtColor(rgb_frame, cv2.COLOR_BGR2RGB)\n",
    "        if self.mode == \"train\":\n",
    "            rgb_t = self.rgb_train_tf(rgb)\n",
    "        else:\n",
    "            rgb_t = self.rgb_val_tf(rgb)\n",
    "\n",
    "        if self.flow_cache_dir is not None:\n",
    "            uv = load_or_compute_flow_stack(\n",
    "                vpath, center_idx, (self.H, self.W),\n",
    "                self.flow_stack, self.flow_method, self.flow_cache_dir\n",
    "            )\n",
    "        else:\n",
    "            frames = read_video_frames(vpath, [center_idx + i for i in range(self.flow_stack+1)],\n",
    "                                       resize_hw=(self.H, self.W))\n",
    "            grays = [cv2.cvtColor(f, cv2.COLOR_BGR2GRAY) for f in frames]\n",
    "            uv = flow_to_uv_stack(grays, self.flow_stack, self.flow_method)\n",
    "        flow_t = torch.from_numpy(uv)  # (2*flow_stack, H, W)\n",
    "\n",
    "        return rgb_t, flow_t, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d706f8",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Định nghĩa `Dataset` trả về bộ **(RGB tensor, FLOW tensor, label)** cho mỗi mẫu video.  \n",
    "- **Đầu vào:** `items` (danh sách video), `img_size`, `flow_stack`, `flow_method`, `flow_cache_dir`.  \n",
    "- **Đầu ra:** `rgb_t` dạng `(3,H,W)`, `flow_t` dạng `(2*FLOW_STACK,H,W)`, `label` (int).  \n",
    "- **Kết quả kỳ vọng:** Dataloader phục vụ 2 stream (RGB & Flow) cho các cơ chế Fusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7096a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = UCF50TwoStreamDataset(\n",
    "    train_list, img_size=cfg.IMG_SIZE, num_segments=cfg.NUM_SEGMENTS,\n",
    "    flow_stack=cfg.FLOW_STACK, flow_method=cfg.FLOW_METHOD,\n",
    "    flow_cache_dir=(cfg.FLOW_CACHE_DIR if cfg.FLOW_PRECOMPUTE else None),\n",
    "    mode=\"train\"\n",
    ")\n",
    "val_ds = UCF50TwoStreamDataset(\n",
    "    val_list, img_size=cfg.IMG_SIZE, num_segments=cfg.NUM_SEGMENTS,\n",
    "    flow_stack=cfg.FLOW_STACK, flow_method=cfg.FLOW_METHOD,\n",
    "    flow_cache_dir=(cfg.FLOW_CACHE_DIR if cfg.FLOW_PRECOMPUTE else None),\n",
    "    mode=\"val\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add6913",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Khởi tạo `Dataset` và `DataLoader` cho train/val.  \n",
    "- **Đầu vào:** `train_list`, `val_list`; cấu hình batch size, workers.  \n",
    "- **Đầu ra:** `train_loader`, `val_loader`.  \n",
    "- **Kết quả kỳ vọng:** Có thể lặp qua batch gồm `(rgb, flow, label)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inflate_conv1_weight(conv1: nn.Conv2d, new_in_channels: int):\n",
    "    '''Mở rộng conv1 để nhận nhiều kênh > 3 bằng cách lặp/avg trọng số ban đầu.'''\n",
    "    old_w = conv1.weight.data  # (out_c, in_c, k, k)\n",
    "    out_c, in_c, kh, kw = old_w.shape\n",
    "    if new_in_channels == in_c:\n",
    "        return conv1\n",
    "    new_w = torch.zeros((out_c, new_in_channels, kh, kw))\n",
    "    for oc in range(out_c):\n",
    "        for ic in range(new_in_channels):\n",
    "            new_w[oc, ic] = old_w[oc, ic % in_c]\n",
    "    conv1.in_channels = new_in_channels\n",
    "    conv1.weight = nn.Parameter(new_w)\n",
    "    return conv1\n",
    "\n",
    "class EarlyChannelResNet(nn.Module):\n",
    "    '''Early-Channel Fusion: ghép RGB + Flow theo kênh => một backbone duy nhất.'''\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.conv1 = inflate_conv1_weight(self.backbone.conv1, in_channels)\n",
    "        in_feat = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_feat, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x: (B, C, H, W)\n",
    "        return self.backbone(x)\n",
    "\n",
    "class TwoBackboneEarlyFeature(nn.Module):\n",
    "    '''Early-Feature Fusion: 2 backbone riêng -> concat features -> classifier.'''\n",
    "    def __init__(self, num_classes, flow_in_channels):\n",
    "        super().__init__()\n",
    "        self.rgb_net = resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.flow_net = resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.flow_net.conv1 = inflate_conv1_weight(self.flow_net.conv1, flow_in_channels)\n",
    "        self.rgb_net.fc = nn.Identity()\n",
    "        self.flow_net.fc = nn.Identity()\n",
    "        in_feat = 512 + 512\n",
    "        self.fc = nn.Linear(in_feat, num_classes)\n",
    "\n",
    "    def forward(self, rgb, flow):\n",
    "        fr = self.rgb_net(rgb)   # (B,512)\n",
    "        ff = self.flow_net(flow) # (B,512)\n",
    "        f = torch.cat([fr, ff], dim=1)\n",
    "        logits = self.fc(f)\n",
    "        return logits\n",
    "\n",
    "class LateFusionModel(nn.Module):\n",
    "    '''Late Fusion: 2 backbone riêng -> logits riêng -> trộn theo trọng số.'''\n",
    "    def __init__(self, num_classes, flow_in_channels, w_rgb=0.5, w_flow=0.5):\n",
    "        super().__init__()\n",
    "        self.rgb_net = resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.flow_net = resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.flow_net.conv1 = inflate_conv1_weight(self.flow_net.conv1, flow_in_channels)\n",
    "        self.rgb_head = nn.Linear(self.rgb_net.fc.in_features, num_classes)\n",
    "        self.flow_head = nn.Linear(self.flow_net.fc.in_features, num_classes)\n",
    "        self.w_rgb = w_rgb\n",
    "        self.w_flow = w_flow\n",
    "        self.rgb_net.fc = nn.Identity()\n",
    "        self.flow_net.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, rgb, flow):\n",
    "        fr = self.rgb_net(rgb)    # (B,512)\n",
    "        ff = self.flow_net(flow)  # (B,512)\n",
    "        lr = self.rgb_head(fr)    # (B,C)\n",
    "        lf = self.flow_head(ff)   # (B,C)\n",
    "        logits = self.w_rgb * lr + self.w_flow * lf\n",
    "        return logits, lr, lf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468ade9",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Khai báo 3 biến thể mô hình Fusion:\n",
    "  - `EarlyChannelResNet`: ghép **RGB + Flow theo kênh** → một backbone.\n",
    "  - `TwoBackboneEarlyFeature`: 2 backbone, **nối đặc trưng** trước classifier.\n",
    "  - `LateFusionModel`: 2 backbone, **trộn logits** theo trọng số `(w_rgb, w_flow)`.\n",
    "- **Đầu vào:** Tensor RGB `(B,3,H,W)`, Tensor Flow `(B,2*FLOW_STACK,H,W)` (tùy biến thể).  \n",
    "- **Đầu ra:** `logits` phân lớp (và `lr, lf` cho late fusion).  \n",
    "- **Kết quả kỳ vọng:** Linh hoạt chọn chiến lược Fusion bằng `cfg.FUSION`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef23c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "flow_in_ch = 2 * cfg.FLOW_STACK\n",
    "\n",
    "if cfg.FUSION == \"early_channel\":\n",
    "    model = EarlyChannelResNet(num_classes=num_classes, in_channels=3 + flow_in_ch).to(device)\n",
    "elif cfg.FUSION == \"early_feature\":\n",
    "    model = TwoBackboneEarlyFeature(num_classes=num_classes, flow_in_channels=flow_in_ch).to(device)\n",
    "elif cfg.FUSION == \"late\":\n",
    "    w_rgb, w_flow = cfg.LATE_FUSION_WEIGHTS\n",
    "    model = LateFusionModel(num_classes=num_classes, flow_in_channels=flow_in_ch,\n",
    "                            w_rgb=w_rgb, w_flow=w_flow).to(device)\n",
    "else:\n",
    "    raise ValueError(\"cfg.FUSION must be one of: early_channel | early_feature | late\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)\n",
    "if cfg.SCHEDULER == \"cosine\":\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.T_MAX)\n",
    "elif cfg.SCHEDULER == \"step\":\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=cfg.STEP_SIZE, gamma=cfg.GAMMA)\n",
    "elif cfg.SCHEDULER == \"plateau\":\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.best = -1.0\n",
    "        self.count = 0\n",
    "    def step(self, metric):\n",
    "        if metric > self.best:\n",
    "            self.best = metric\n",
    "            self.count = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.count += 1\n",
    "            return False\n",
    "    def should_stop(self):\n",
    "        return self.count >= self.patience\n",
    "\n",
    "early_stopper = EarlyStopper(cfg.EARLY_STOP_PATIENCE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef6521",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Khởi tạo mô hình theo `cfg.FUSION`, cùng **optimizer**, **scheduler**, và **early stopping**.  \n",
    "- **Đầu vào:** `cfg` (hệ số, kiểu fusion, learning rate, v.v.).  \n",
    "- **Đầu ra:** `model`, `optimizer`, `scheduler`, `criterion`, `early_stopper`.  \n",
    "- **Kết quả kỳ vọng:** Sẵn sàng huấn luyện theo chiến lược đã chọn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for rgb, flow, y in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        rgb = rgb.to(device)\n",
    "        flow = flow.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(model, EarlyChannelResNet):\n",
    "            x = torch.cat([rgb, flow], dim=1)  # (B, 3+2*flow_stack, H, W)\n",
    "            logits = model(x)\n",
    "        elif isinstance(model, TwoBackboneEarlyFeature):\n",
    "            logits = model(rgb, flow)\n",
    "        elif isinstance(model, LateFusionModel):\n",
    "            logits, _, _ = model(rgb, flow)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown model type\")\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    all_y, all_pred = [], []\n",
    "    for rgb, flow, y in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        rgb = rgb.to(device)\n",
    "        flow = flow.to(device)\n",
    "        y = y.to(device)\n",
    "        if isinstance(model, EarlyChannelResNet):\n",
    "            x = torch.cat([rgb, flow], dim=1)\n",
    "            logits = model(x)\n",
    "        elif isinstance(model, TwoBackboneEarlyFeature):\n",
    "            logits = model(rgb, flow)\n",
    "        elif isinstance(model, LateFusionModel):\n",
    "            logits, _, _ = model(rgb, flow)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown model type\")\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        pred = logits.argmax(dim=1)\n",
    "\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "\n",
    "        all_y.extend(y.cpu().numpy().tolist())\n",
    "        all_pred.extend(pred.cpu().numpy().tolist())\n",
    "\n",
    "    acc = correct/total if total > 0 else 0.0\n",
    "    return loss_sum/max(total,1), acc, np.array(all_y), np.array(all_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d0b54",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Vòng lặp **train** và **evaluate** chia sẻ logic cho ba biến thể model.  \n",
    "- **Đầu vào:** `model`, `DataLoader`, `optimizer`, `criterion`.  \n",
    "- **Đầu ra:** Loss trung bình, Accuracy, (ở `evaluate` có thêm `y_true`, `y_pred`).  \n",
    "- **Kết quả kỳ vọng:** Theo dõi tiến trình huấn luyện và tổng hợp chỉ số.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_acc = -1.0\n",
    "log_hist = []\n",
    "\n",
    "for epoch in range(1, cfg.NUM_EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, y_true, y_pred = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    if cfg.SCHEDULER == \"plateau\":\n",
    "        scheduler.step(val_acc)\n",
    "    elif cfg.SCHEDULER in [\"cosine\", \"step\"] and scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    rec = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"time_sec\": round(time.time()-t0, 2)\n",
    "    }\n",
    "    log_hist.append(rec)\n",
    "    print(f\"[Epoch {epoch:02d}] \"\n",
    "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | lr={rec['lr']:.2e} | \"\n",
    "          f\"time={rec['time_sec']}s\")\n",
    "\n",
    "    improved = val_acc > best_acc\n",
    "    if improved and cfg.SAVE_BEST:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), os.path.join(cfg.OUTPUT_DIR, cfg.BEST_MODEL_PATH))\n",
    "        with open(os.path.join(cfg.OUTPUT_DIR, \"best_epoch.txt\"), \"w\") as f:\n",
    "            f.write(f\"best_epoch={epoch}\\nval_acc={val_acc:.4f}\\n\")\n",
    "\n",
    "    if early_stopper.step(val_acc) is False and early_stopper.should_stop():\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, cfg.LOG_JSON), \"w\") as f:\n",
    "    json.dump(log_hist, f, indent=2)\n",
    "\n",
    "print(\"Training done. Best val acc:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca54f1",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Tiến hành huấn luyện đầy đủ, theo dõi loss/accuracy, cập nhật LR, lưu **best model**, và **early stopping**.  \n",
    "- **Đầu vào:** Cấu hình và các thành phần đã khởi tạo trước đó.  \n",
    "- **Đầu ra:** File `best_model.pth`, `best_epoch.txt`, và log `train_log.json` trong `OUTPUT_DIR`.  \n",
    "- **Kết quả kỳ vọng:** Huấn luyện dừng sớm nếu không tiến bộ; bạn có mô hình tốt nhất trên tập validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_and_report(model, loader, device, class_names):\n",
    "    model.eval()\n",
    "    all_y, all_pred = [], []\n",
    "    for rgb, flow, y in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        rgb = rgb.to(device)\n",
    "        flow = flow.to(device)\n",
    "        y = y.to(device)\n",
    "        if isinstance(model, EarlyChannelResNet):\n",
    "            x = torch.cat([rgb, flow], dim=1)\n",
    "            logits = model(x)\n",
    "        elif isinstance(model, TwoBackboneEarlyFeature):\n",
    "            logits = model(rgb, flow)\n",
    "        elif isinstance(model, LateFusionModel):\n",
    "            logits, _, _ = model(rgb, flow)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        all_y.extend(y.cpu().numpy().tolist())\n",
    "        all_pred.extend(pred.cpu().numpy().tolist())\n",
    "    all_y = np.array(all_y)\n",
    "    all_pred = np.array(all_pred)\n",
    "    print(classification_report(all_y, all_pred, target_names=class_names, digits=3))\n",
    "    cm = confusion_matrix(all_y, all_pred, labels=list(range(len(class_names))))\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return cm\n",
    "\n",
    "best_path = os.path.join(cfg.OUTPUT_DIR, cfg.BEST_MODEL_PATH)\n",
    "if os.path.exists(best_path):\n",
    "    state = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    _ = evaluate_and_report(model, val_loader, device, classes)\n",
    "else:\n",
    "    print(\"Best model not found; please train first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92faeb",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Sinh **classification report** (precision/recall/F1) và **Confusion Matrix**.  \n",
    "- **Đầu vào:** `model` đã huấn luyện, `val_loader`, danh sách `classes`.  \n",
    "- **Đầu ra:** In báo cáo và vẽ ma trận nhầm lẫn.  \n",
    "- **Kết quả kỳ vọng:** Đánh giá trực quan, nhận biết lớp nào hay nhầm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def predict_single_video(model, video_path, cfg):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.isOpened() else 2\n",
    "    cap.release()\n",
    "    ci = min(max(total//2, 1), max(total-2, 1))\n",
    "\n",
    "    rgb_frame = read_video_frames(video_path, [ci], resize_hw=(cfg.IMG_SIZE, cfg.IMG_SIZE))[0]\n",
    "    rgb = cv2.cvtColor(rgb_frame, cv2.COLOR_BGR2RGB)\n",
    "    tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    rgb_t = tf(rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    uv = load_or_compute_flow_stack(\n",
    "        video_path, ci, (cfg.IMG_SIZE, cfg.IMG_SIZE), cfg.FLOW_STACK, cfg.FLOW_METHOD, cfg.FLOW_CACHE_DIR\n",
    "    )\n",
    "    flow_t = torch.from_numpy(uv).unsqueeze(0).to(device)  # (1,2*stack,H,W)\n",
    "\n",
    "    if isinstance(model, EarlyChannelResNet):\n",
    "        x = torch.cat([rgb_t, flow_t], dim=1)\n",
    "        logits = model(x)\n",
    "    elif isinstance(model, TwoBackboneEarlyFeature):\n",
    "        logits = model(rgb_t, flow_t)\n",
    "    elif isinstance(model, LateFusionModel):\n",
    "        logits, _, _ = model(rgb_t, flow_t)\n",
    "\n",
    "    prob = F.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    pred_idx = int(np.argmax(prob))\n",
    "    return pred_idx, prob\n",
    "\n",
    "# Ví dụ sử dụng (sau khi train):\n",
    "# pred_idx, prob = predict_single_video(model, \"/kaggle/input/ucf50/UCF50/BenchPress/v_BenchPress_g08_c02.avi\", cfg)\n",
    "# print(\"Dự đoán:\", classes[pred_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305da4af",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Suy luận trên **một video** duy nhất, trả về lớp dự đoán và phân bố xác suất.  \n",
    "- **Đầu vào:** `video_path` tới tệp `.avi`, `cfg`.  \n",
    "- **Đầu ra:** `pred_idx` (int), `prob` (mảng xác suất).  \n",
    "- **Kết quả kỳ vọng:** Dễ dàng kiểm thử nhanh một video bất kỳ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e25b4b",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Tips\n",
    "\n",
    "- **Cấu trúc dữ liệu UCF50** mong đợi:\n",
    "  ```\n",
    "  UCF50/\n",
    "    ClassA/\n",
    "      video1.avi\n",
    "      video2.avi\n",
    "      ...\n",
    "    ClassB/\n",
    "      v_*.avi\n",
    "      ...\n",
    "    ...\n",
    "  ```\n",
    "- **Tốc độ:** Tính Optical Flow on-the-fly là nặng. Hãy cân nhắc:\n",
    "  - Giảm `FLOW_STACK` (ví dụ, 3)\n",
    "  - Giảm `IMG_SIZE` (ví dụ, 168 hoặc 128)\n",
    "  - Chuyển `FLOW_METHOD` sang `\"farneback\"` (nhanh hơn TV-L1)\n",
    "  - **Bật cache** (`FLOW_PRECOMPUTE=True`), notebook sẽ tự lưu/tải `.npy`.\n",
    "- **Ký hiệu Fusion:**\n",
    "  - *Early-Channel*: đơn giản, nhanh inference; nhưng phụ thuộc sửa conv1.\n",
    "  - *Early-Feature*: linh hoạt ở mức đặc trưng; classifier chung.\n",
    "  - *Late*: đơn giản, có thể huấn luyện 2 nhánh độc lập; dễ điều chỉnh trọng số.\n",
    "- **Scheduler & EarlyStopping:**\n",
    "  - `cosine` thường mượt; `step` giúp giảm LR theo mốc; `plateau` bám theo `val_acc`.\n",
    "  - `EARLY_STOP_PATIENCE` nên chọn 3–8 tùy dữ liệu.\n",
    "- **Đánh giá:** UCF50 hay dùng cross-subject splits; ở đây ta **stratified random split** để đơn giản.\n",
    "- **Tái lập:** cố định `SEED`; chú ý `cudnn.benchmark=False` để nhất quán.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27469c5f",
   "metadata": {},
   "source": [
    "\n",
    "**Giải thích:**  \n",
    "- **Mục đích:** Ghi chú vận hành, tối ưu và giải thích các biến thể fusion.  \n",
    "- **Đầu vào:** Không.  \n",
    "- **Đầu ra:** Không.  \n",
    "- **Kết quả kỳ vọng:** Bạn có đủ gợi ý để tùy chỉnh cho máy và mục tiêu của mình.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
