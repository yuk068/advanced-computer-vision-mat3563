{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc26b8c-8a11-4529-868f-ff960230b36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    " !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad22965a-fc87-4863-a82f-117227637ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed tensors: []\n",
      "New tensors: ['bn1.num_batches_tracked', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.bn3.num_batches_tracked']\n",
      "Following layers will be skipped: ['fc']\n",
      "Initialized layers: ['conv1.weight', 'bn1.running_mean', 'bn1.running_var', 'bn1.weight', 'bn1.bias', 'layer1.0.conv1.weight', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.conv2.weight', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.conv3.weight', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.1.conv1.weight', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.conv2.weight', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.conv3.weight', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.2.conv1.weight', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.conv2.weight', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.conv3.weight', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer2.0.conv1.weight', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.conv2.weight', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.conv3.weight', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.1.conv1.weight', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.conv2.weight', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.conv3.weight', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.2.conv1.weight', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.conv2.weight', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.conv3.weight', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.3.conv1.weight', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.conv2.weight', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.conv3.weight', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer3.0.conv1.weight', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.conv2.weight', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.conv3.weight', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.1.conv1.weight', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.conv2.weight', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.conv3.weight', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.2.conv1.weight', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.conv2.weight', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.conv3.weight', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.3.conv1.weight', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.conv2.weight', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.conv3.weight', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.4.conv1.weight', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.conv2.weight', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.conv3.weight', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.5.conv1.weight', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.conv2.weight', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.conv3.weight', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer4.0.conv1.weight', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.conv2.weight', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.conv3.weight', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.1.conv1.weight', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.conv2.weight', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.conv3.weight', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.2.conv1.weight', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.conv2.weight', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.conv3.weight', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias']\n",
      "Uninitialized layers: ['bn1.num_batches_tracked', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.bn3.num_batches_tracked', 'fc.weight', 'fc.bias']\n",
      "Unused layers: ['fc.weight', 'fc.bias']\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1     [-1, 64, 64, 112, 112]          28,224\n",
      "       BatchNorm3d-2     [-1, 64, 64, 112, 112]             128\n",
      "              ReLU-3     [-1, 64, 64, 112, 112]               0\n",
      "         MaxPool3d-4       [-1, 64, 64, 56, 56]               0\n",
      "            Conv3d-5       [-1, 64, 64, 56, 56]           4,096\n",
      "       BatchNorm3d-6       [-1, 64, 64, 56, 56]             128\n",
      "              ReLU-7       [-1, 64, 64, 56, 56]               0\n",
      "            Conv3d-8       [-1, 64, 64, 56, 56]         110,592\n",
      "       BatchNorm3d-9       [-1, 64, 64, 56, 56]             128\n",
      "             ReLU-10       [-1, 64, 64, 56, 56]               0\n",
      "           Conv3d-11      [-1, 256, 64, 56, 56]          16,384\n",
      "      BatchNorm3d-12      [-1, 256, 64, 56, 56]             512\n",
      "           Conv3d-13      [-1, 256, 64, 56, 56]          16,384\n",
      "      BatchNorm3d-14      [-1, 256, 64, 56, 56]             512\n",
      "             ReLU-15      [-1, 256, 64, 56, 56]               0\n",
      "       Bottleneck-16      [-1, 256, 64, 56, 56]               0\n",
      "           Conv3d-17       [-1, 64, 64, 56, 56]          16,384\n",
      "      BatchNorm3d-18       [-1, 64, 64, 56, 56]             128\n",
      "             ReLU-19       [-1, 64, 64, 56, 56]               0\n",
      "           Conv3d-20       [-1, 64, 64, 56, 56]         110,592\n",
      "      BatchNorm3d-21       [-1, 64, 64, 56, 56]             128\n",
      "             ReLU-22       [-1, 64, 64, 56, 56]               0\n",
      "           Conv3d-23      [-1, 256, 64, 56, 56]          16,384\n",
      "      BatchNorm3d-24      [-1, 256, 64, 56, 56]             512\n",
      "             ReLU-25      [-1, 256, 64, 56, 56]               0\n",
      "       Bottleneck-26      [-1, 256, 64, 56, 56]               0\n",
      "           Conv3d-27       [-1, 64, 64, 56, 56]          16,384\n",
      "      BatchNorm3d-28       [-1, 64, 64, 56, 56]             128\n",
      "             ReLU-29       [-1, 64, 64, 56, 56]               0\n",
      "           Conv3d-30       [-1, 64, 64, 56, 56]         110,592\n",
      "      BatchNorm3d-31       [-1, 64, 64, 56, 56]             128\n",
      "             ReLU-32       [-1, 64, 64, 56, 56]               0\n",
      "           Conv3d-33      [-1, 256, 64, 56, 56]          16,384\n",
      "      BatchNorm3d-34      [-1, 256, 64, 56, 56]             512\n",
      "             ReLU-35      [-1, 256, 64, 56, 56]               0\n",
      "       Bottleneck-36      [-1, 256, 64, 56, 56]               0\n",
      "           Conv3d-37      [-1, 128, 64, 56, 56]          32,768\n",
      "      BatchNorm3d-38      [-1, 128, 64, 56, 56]             256\n",
      "             ReLU-39      [-1, 128, 64, 56, 56]               0\n",
      "           Conv3d-40      [-1, 128, 32, 28, 28]         442,368\n",
      "      BatchNorm3d-41      [-1, 128, 32, 28, 28]             256\n",
      "             ReLU-42      [-1, 128, 32, 28, 28]               0\n",
      "           Conv3d-43      [-1, 512, 32, 28, 28]          65,536\n",
      "      BatchNorm3d-44      [-1, 512, 32, 28, 28]           1,024\n",
      "           Conv3d-45      [-1, 512, 32, 28, 28]         131,072\n",
      "      BatchNorm3d-46      [-1, 512, 32, 28, 28]           1,024\n",
      "             ReLU-47      [-1, 512, 32, 28, 28]               0\n",
      "       Bottleneck-48      [-1, 512, 32, 28, 28]               0\n",
      "           Conv3d-49      [-1, 128, 32, 28, 28]          65,536\n",
      "      BatchNorm3d-50      [-1, 128, 32, 28, 28]             256\n",
      "             ReLU-51      [-1, 128, 32, 28, 28]               0\n",
      "           Conv3d-52      [-1, 128, 32, 28, 28]         442,368\n",
      "      BatchNorm3d-53      [-1, 128, 32, 28, 28]             256\n",
      "             ReLU-54      [-1, 128, 32, 28, 28]               0\n",
      "           Conv3d-55      [-1, 512, 32, 28, 28]          65,536\n",
      "      BatchNorm3d-56      [-1, 512, 32, 28, 28]           1,024\n",
      "             ReLU-57      [-1, 512, 32, 28, 28]               0\n",
      "       Bottleneck-58      [-1, 512, 32, 28, 28]               0\n",
      "           Conv3d-59      [-1, 128, 32, 28, 28]          65,536\n",
      "      BatchNorm3d-60      [-1, 128, 32, 28, 28]             256\n",
      "             ReLU-61      [-1, 128, 32, 28, 28]               0\n",
      "           Conv3d-62      [-1, 128, 32, 28, 28]         442,368\n",
      "      BatchNorm3d-63      [-1, 128, 32, 28, 28]             256\n",
      "             ReLU-64      [-1, 128, 32, 28, 28]               0\n",
      "           Conv3d-65      [-1, 512, 32, 28, 28]          65,536\n",
      "      BatchNorm3d-66      [-1, 512, 32, 28, 28]           1,024\n",
      "             ReLU-67      [-1, 512, 32, 28, 28]               0\n",
      "       Bottleneck-68      [-1, 512, 32, 28, 28]               0\n",
      "           Conv3d-69      [-1, 128, 32, 28, 28]          65,536\n",
      "      BatchNorm3d-70      [-1, 128, 32, 28, 28]             256\n",
      "             ReLU-71      [-1, 128, 32, 28, 28]               0\n",
      "           Conv3d-72      [-1, 128, 32, 28, 28]         442,368\n",
      "      BatchNorm3d-73      [-1, 128, 32, 28, 28]             256\n",
      "             ReLU-74      [-1, 128, 32, 28, 28]               0\n",
      "           Conv3d-75      [-1, 512, 32, 28, 28]          65,536\n",
      "      BatchNorm3d-76      [-1, 512, 32, 28, 28]           1,024\n",
      "             ReLU-77      [-1, 512, 32, 28, 28]               0\n",
      "       Bottleneck-78      [-1, 512, 32, 28, 28]               0\n",
      "           Conv3d-79      [-1, 256, 32, 28, 28]         131,072\n",
      "      BatchNorm3d-80      [-1, 256, 32, 28, 28]             512\n",
      "             ReLU-81      [-1, 256, 32, 28, 28]               0\n",
      "           Conv3d-82      [-1, 256, 16, 14, 14]       1,769,472\n",
      "      BatchNorm3d-83      [-1, 256, 16, 14, 14]             512\n",
      "             ReLU-84      [-1, 256, 16, 14, 14]               0\n",
      "           Conv3d-85     [-1, 1024, 16, 14, 14]         262,144\n",
      "      BatchNorm3d-86     [-1, 1024, 16, 14, 14]           2,048\n",
      "           Conv3d-87     [-1, 1024, 16, 14, 14]         524,288\n",
      "      BatchNorm3d-88     [-1, 1024, 16, 14, 14]           2,048\n",
      "             ReLU-89     [-1, 1024, 16, 14, 14]               0\n",
      "       Bottleneck-90     [-1, 1024, 16, 14, 14]               0\n",
      "           Conv3d-91      [-1, 256, 16, 14, 14]         262,144\n",
      "      BatchNorm3d-92      [-1, 256, 16, 14, 14]             512\n",
      "             ReLU-93      [-1, 256, 16, 14, 14]               0\n",
      "           Conv3d-94      [-1, 256, 16, 14, 14]       1,769,472\n",
      "      BatchNorm3d-95      [-1, 256, 16, 14, 14]             512\n",
      "             ReLU-96      [-1, 256, 16, 14, 14]               0\n",
      "           Conv3d-97     [-1, 1024, 16, 14, 14]         262,144\n",
      "      BatchNorm3d-98     [-1, 1024, 16, 14, 14]           2,048\n",
      "             ReLU-99     [-1, 1024, 16, 14, 14]               0\n",
      "      Bottleneck-100     [-1, 1024, 16, 14, 14]               0\n",
      "          Conv3d-101      [-1, 256, 16, 14, 14]         262,144\n",
      "     BatchNorm3d-102      [-1, 256, 16, 14, 14]             512\n",
      "            ReLU-103      [-1, 256, 16, 14, 14]               0\n",
      "          Conv3d-104      [-1, 256, 16, 14, 14]       1,769,472\n",
      "     BatchNorm3d-105      [-1, 256, 16, 14, 14]             512\n",
      "            ReLU-106      [-1, 256, 16, 14, 14]               0\n",
      "          Conv3d-107     [-1, 1024, 16, 14, 14]         262,144\n",
      "     BatchNorm3d-108     [-1, 1024, 16, 14, 14]           2,048\n",
      "            ReLU-109     [-1, 1024, 16, 14, 14]               0\n",
      "      Bottleneck-110     [-1, 1024, 16, 14, 14]               0\n",
      "          Conv3d-111      [-1, 256, 16, 14, 14]         262,144\n",
      "     BatchNorm3d-112      [-1, 256, 16, 14, 14]             512\n",
      "            ReLU-113      [-1, 256, 16, 14, 14]               0\n",
      "          Conv3d-114      [-1, 256, 16, 14, 14]       1,769,472\n",
      "     BatchNorm3d-115      [-1, 256, 16, 14, 14]             512\n",
      "            ReLU-116      [-1, 256, 16, 14, 14]               0\n",
      "          Conv3d-117     [-1, 1024, 16, 14, 14]         262,144\n",
      "     BatchNorm3d-118     [-1, 1024, 16, 14, 14]           2,048\n",
      "            ReLU-119     [-1, 1024, 16, 14, 14]               0\n",
      "      Bottleneck-120     [-1, 1024, 16, 14, 14]               0\n",
      "          Conv3d-121      [-1, 256, 16, 14, 14]         262,144\n",
      "     BatchNorm3d-122      [-1, 256, 16, 14, 14]             512\n",
      "            ReLU-123      [-1, 256, 16, 14, 14]               0\n",
      "          Conv3d-124      [-1, 256, 16, 14, 14]       1,769,472\n",
      "     BatchNorm3d-125      [-1, 256, 16, 14, 14]             512\n",
      "            ReLU-126      [-1, 256, 16, 14, 14]               0\n",
      "          Conv3d-127     [-1, 1024, 16, 14, 14]         262,144\n",
      "     BatchNorm3d-128     [-1, 1024, 16, 14, 14]           2,048\n",
      "            ReLU-129     [-1, 1024, 16, 14, 14]               0\n",
      "      Bottleneck-130     [-1, 1024, 16, 14, 14]               0\n",
      "          Conv3d-131      [-1, 256, 16, 14, 14]         262,144\n",
      "     BatchNorm3d-132      [-1, 256, 16, 14, 14]             512\n",
      "            ReLU-133      [-1, 256, 16, 14, 14]               0\n",
      "          Conv3d-134      [-1, 256, 16, 14, 14]       1,769,472\n",
      "     BatchNorm3d-135      [-1, 256, 16, 14, 14]             512\n",
      "            ReLU-136      [-1, 256, 16, 14, 14]               0\n",
      "          Conv3d-137     [-1, 1024, 16, 14, 14]         262,144\n",
      "     BatchNorm3d-138     [-1, 1024, 16, 14, 14]           2,048\n",
      "            ReLU-139     [-1, 1024, 16, 14, 14]               0\n",
      "      Bottleneck-140     [-1, 1024, 16, 14, 14]               0\n",
      "          Conv3d-141      [-1, 512, 16, 14, 14]         524,288\n",
      "     BatchNorm3d-142      [-1, 512, 16, 14, 14]           1,024\n",
      "            ReLU-143      [-1, 512, 16, 14, 14]               0\n",
      "          Conv3d-144         [-1, 512, 8, 7, 7]       7,077,888\n",
      "     BatchNorm3d-145         [-1, 512, 8, 7, 7]           1,024\n",
      "            ReLU-146         [-1, 512, 8, 7, 7]               0\n",
      "          Conv3d-147        [-1, 2048, 8, 7, 7]       1,048,576\n",
      "     BatchNorm3d-148        [-1, 2048, 8, 7, 7]           4,096\n",
      "          Conv3d-149        [-1, 2048, 8, 7, 7]       2,097,152\n",
      "     BatchNorm3d-150        [-1, 2048, 8, 7, 7]           4,096\n",
      "            ReLU-151        [-1, 2048, 8, 7, 7]               0\n",
      "      Bottleneck-152        [-1, 2048, 8, 7, 7]               0\n",
      "          Conv3d-153         [-1, 512, 8, 7, 7]       1,048,576\n",
      "     BatchNorm3d-154         [-1, 512, 8, 7, 7]           1,024\n",
      "            ReLU-155         [-1, 512, 8, 7, 7]               0\n",
      "          Conv3d-156         [-1, 512, 8, 7, 7]       7,077,888\n",
      "     BatchNorm3d-157         [-1, 512, 8, 7, 7]           1,024\n",
      "            ReLU-158         [-1, 512, 8, 7, 7]               0\n",
      "          Conv3d-159        [-1, 2048, 8, 7, 7]       1,048,576\n",
      "     BatchNorm3d-160        [-1, 2048, 8, 7, 7]           4,096\n",
      "            ReLU-161        [-1, 2048, 8, 7, 7]               0\n",
      "      Bottleneck-162        [-1, 2048, 8, 7, 7]               0\n",
      "          Conv3d-163         [-1, 512, 8, 7, 7]       1,048,576\n",
      "     BatchNorm3d-164         [-1, 512, 8, 7, 7]           1,024\n",
      "            ReLU-165         [-1, 512, 8, 7, 7]               0\n",
      "          Conv3d-166         [-1, 512, 8, 7, 7]       7,077,888\n",
      "     BatchNorm3d-167         [-1, 512, 8, 7, 7]           1,024\n",
      "            ReLU-168         [-1, 512, 8, 7, 7]               0\n",
      "          Conv3d-169        [-1, 2048, 8, 7, 7]       1,048,576\n",
      "     BatchNorm3d-170        [-1, 2048, 8, 7, 7]           4,096\n",
      "            ReLU-171        [-1, 2048, 8, 7, 7]               0\n",
      "      Bottleneck-172        [-1, 2048, 8, 7, 7]               0\n",
      "         Dropout-173                 [-1, 2048]               0\n",
      "          Linear-174                  [-1, 400]         819,600\n",
      "================================================================\n",
      "Total params: 46,980,944\n",
      "Trainable params: 46,980,944\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 36.75\n",
      "Forward/backward pass size (MB): 12524.11\n",
      "Params size (MB): 179.22\n",
      "Estimated Total Size (MB): 12740.08\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model I3D-ResNet\n",
    "inflate-Resnet-3D CNN\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def inflate_from_2d_model(state_dict_2d, state_dict_3d, skipped_keys=None, inflated_dim=2):\n",
    "\n",
    "    if skipped_keys is None:\n",
    "        skipped_keys = []\n",
    "\n",
    "    missed_keys = []\n",
    "    new_keys = []\n",
    "    for old_key in state_dict_2d.keys():\n",
    "        if old_key not in state_dict_3d.keys():\n",
    "            missed_keys.append(old_key)\n",
    "    for new_key in state_dict_3d.keys():\n",
    "        if new_key not in state_dict_2d.keys():\n",
    "            new_keys.append(new_key)\n",
    "    print(\"Missed tensors: {}\".format(missed_keys))\n",
    "    print(\"New tensors: {}\".format(new_keys))\n",
    "    print(\"Following layers will be skipped: {}\".format(skipped_keys))\n",
    "\n",
    "    state_d = OrderedDict()\n",
    "    unused_layers = [k for k in state_dict_2d.keys()]\n",
    "    uninitialized_layers = [k for k in state_dict_3d.keys()]\n",
    "    initialized_layers = []\n",
    "    for key, value in state_dict_2d.items():\n",
    "        skipped = False\n",
    "        for skipped_key in skipped_keys:\n",
    "            if skipped_key in key:\n",
    "                skipped = True\n",
    "                break\n",
    "        if skipped:\n",
    "            continue\n",
    "        new_value = value\n",
    "        # only inflated conv's weights\n",
    "        if key in state_dict_3d:\n",
    "            if value.ndimension() == 4 and 'weight' in key:\n",
    "                value = torch.unsqueeze(value, inflated_dim)\n",
    "                repeated_dim = torch.ones(state_dict_3d[key].ndimension(), dtype=torch.int)\n",
    "                repeated_dim[inflated_dim] = state_dict_3d[key].size(inflated_dim)\n",
    "                new_value = value.repeat(repeated_dim.tolist())\n",
    "            state_d[key] = new_value\n",
    "            initialized_layers.append(key)\n",
    "            uninitialized_layers.remove(key)\n",
    "            unused_layers.remove(key)\n",
    "\n",
    "    print(\"Initialized layers: {}\".format(initialized_layers))\n",
    "    print(\"Uninitialized layers: {}\".format(uninitialized_layers))\n",
    "    print(\"Unused layers: {}\".format(unused_layers))\n",
    "\n",
    "    return state_d\n",
    "\n",
    "################################################################################\n",
    "__all__ = ['i3d_resnet']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def BasicConv3d(in_planes, out_planes, kernel_size, stride=(1, 1, 1), padding=(0, 0, 0),\n",
    "                bias=False):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                     stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=(1, 1, 1), padding=0, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = BasicConv3d(inplanes, planes, kernel_size=(3, 3, 3),\n",
    "                                 stride=stride, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = BasicConv3d(planes, planes, kernel_size=3,\n",
    "                                 stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, padding=0, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = BasicConv3d(inplanes, planes, kernel_size=(1, 1, 1),\n",
    "                                 stride=(1, 1, 1), padding=(0, 0, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = BasicConv3d(planes, planes, kernel_size=(3, 3, 3),\n",
    "                                 stride=stride, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = BasicConv3d(planes, planes * self.expansion, kernel_size=(1, 1, 1),\n",
    "                                 stride=(1, 1, 1), padding=(0, 0, 0), bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class I3D_ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes=1000, dropout=0.5, without_t_stride=False,\n",
    "                 zero_init_residual=False):\n",
    "        super(I3D_ResNet, self).__init__()\n",
    "        layers = {\n",
    "            18: [2, 2, 2, 2],\n",
    "            34: [3, 4, 6, 3],\n",
    "            50: [3, 4, 6, 3],\n",
    "            101: [3, 4, 23, 3],\n",
    "            152: [3, 8, 36, 3]}[depth]\n",
    "        block = BasicBlock if depth < 50 else Bottleneck\n",
    "        self.depth = depth\n",
    "        self.without_t_stride = without_t_stride\n",
    "        self.inplanes = 64\n",
    "        self.t_s = 1 if without_t_stride else 2\n",
    "        self.conv1 = BasicConv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3),\n",
    "                                 bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.001)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "    \n",
    "    def mean(self, modality='rgb'):\n",
    "        return [0.485, 0.456, 0.406] if modality == 'rgb' else [0.5]\n",
    "\n",
    "    def std(self, modality='rgb'):\n",
    "        return [0.229, 0.224, 0.225] if modality == 'rgb' else [np.mean([0.229, 0.224, 0.225])]\n",
    "    \n",
    "    @property\n",
    "    def network_name(self):\n",
    "        name = 'i3d-resnet-{}'.format(self.depth)\n",
    "\n",
    "        if not self.without_t_stride:\n",
    "            name += '-ts'.format(self.depth)\n",
    "        return name\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                BasicConv3d(self.inplanes, planes * block.expansion, kernel_size=(1, 1, 1),\n",
    "                            stride=(self.t_s if stride == 2 else 1, stride, stride)),\n",
    "                nn.BatchNorm3d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride=(self.t_s if stride == 2 else 1, stride, stride),\n",
    "                            padding=1, downsample=downsample))\n",
    "\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, padding=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        num_frames = x.shape[2]\n",
    "        x = F.adaptive_avg_pool3d(x, output_size=(num_frames, 1, 1))\n",
    "        # N x 1024 x ((F/8)-1) x 1 x 1\n",
    "        x = x.squeeze(-1)\n",
    "        x = x.squeeze(-1)\n",
    "        x = x.transpose(1, 2)\n",
    "        n, c, nf = x.size()\n",
    "        x = x.contiguous().view(n * c, -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(n, c, -1)\n",
    "        # N x num_classes x ((F/8)-1)\n",
    "        logits = torch.mean(x, 1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def i3d_resnet(depth, num_classes, dropout, without_t_stride, **kwargs):\n",
    "    model = I3D_ResNet(depth, num_classes=num_classes, dropout=dropout, without_t_stride=without_t_stride)\n",
    "\n",
    "    new_model_state_dict = model.state_dict()\n",
    "    state_dict = model_zoo.load_url(model_urls['resnet{}'.format(depth)],\n",
    "                                    map_location='cpu', progress=True)\n",
    "    state_d = inflate_from_2d_model(state_dict, new_model_state_dict,\n",
    "                                    skipped_keys=['fc'])\n",
    "    model.load_state_dict(state_d, strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from torchsummary import torchsummary\n",
    "    model = i3d_resnet(50, 400, 0.5, without_t_stride=False)\n",
    "\n",
    "    dummy_data = (3, 64, 224, 224)\n",
    "    model.eval()\n",
    "    model_summary = torchsummary.summary(model, input_size=dummy_data)\n",
    "    print(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32eb6f03-6111-4031-806d-cd54ee5c7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model_Builder\n",
    "\"\"\"\n",
    "\n",
    "MODEL_TABLE = {\n",
    "    'i3d_resnet': i3d_resnet,\n",
    "}\n",
    "\n",
    "\n",
    "def build_model(args, test_mode=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        args: all options defined in opts.py and num_classes\n",
    "        test_mode:\n",
    "    Returns:\n",
    "        network model\n",
    "        architecture name\n",
    "    \"\"\"\n",
    "    model = MODEL_TABLE[args.backbone_net](**vars(args))\n",
    "    network_name = model.network_name if hasattr(model, 'network_name') else args.backbone_net\n",
    "    arch_name = \"{dataset}-{modality}-{arch_name}\".format(\n",
    "        dataset=args.dataset, modality=args.modality, arch_name=network_name)\n",
    "    arch_name += \"-f{}\".format(args.groups)\n",
    "\n",
    "    # add setting info only in training\n",
    "    if not test_mode:\n",
    "        arch_name += \"-{}{}-bs{}-e{}\".format(args.lr_scheduler, \"-syncbn\" if args.sync_bn else \"\",\n",
    "                                             args.batch_size, args.epochs)\n",
    "    return model, arch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea3cd2-dbc6-4044-a00e-96db8d1ca926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584ac08-8e8c-493b-9cdd-67c0ee337706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import lr_scheduler\n",
    "import tensorboard_logger\n",
    "\n",
    "from models import build_model\n",
    "from utils.utils import (train, validate, build_dataflow, get_augmentor,\n",
    "                         save_checkpoint)\n",
    "from utils.video_dataset import VideoDataSet\n",
    "from utils.dataset_config import get_dataset_config\n",
    "from opts import arg_parser\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args\n",
    "    parser = arg_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    if args.multiprocessing_distributed:\n",
    "        # Since we have ngpus_per_node processes per node, the total world_size\n",
    "        # needs to be adjusted accordingly\n",
    "        args.world_size = ngpus_per_node * args.world_size\n",
    "        # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
    "        # main_worker process function\n",
    "        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    "    else:\n",
    "        # Simply call main_worker function\n",
    "        main_worker(args.gpu, ngpus_per_node, args)\n",
    "\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    cudnn.benchmark = args.cudnn_benchmark\n",
    "    args.gpu = gpu\n",
    "\n",
    "    num_classes, train_list_name, val_list_name, test_list_name, filename_seperator, image_tmpl, filter_video, label_file = get_dataset_config(\n",
    "        args.dataset)\n",
    "    args.num_classes = num_classes\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.dist_url == \"env://\" and args.rank == -1:\n",
    "            args.rank = int(os.environ[\"RANK\"])\n",
    "        if args.multiprocessing_distributed:\n",
    "            # For multiprocessing distributed training, rank needs to be the\n",
    "            # global rank among all the processes\n",
    "            args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "    if args.modality == 'rgb':\n",
    "        args.input_channels = 3\n",
    "    elif args.modality == 'flow':\n",
    "        args.input_channels = 2 * 5\n",
    "\n",
    "    model, arch_name = build_model(args)\n",
    "    mean = model.mean(args.modality)\n",
    "    std = model.std(args.modality)\n",
    "\n",
    "    # overwrite mean and std if they are presented in command\n",
    "    if args.mean is not None:\n",
    "        if args.modality == 'rgb':\n",
    "            if len(args.mean) != 3:\n",
    "                raise ValueError(\"When training with rgb, dim of mean must be three.\")\n",
    "        elif args.modality == 'flow':\n",
    "            if len(args.mean) != 1:\n",
    "                raise ValueError(\"When training with flow, dim of mean must be three.\")\n",
    "        mean = args.mean\n",
    "\n",
    "    if args.std is not None:\n",
    "        if args.modality == 'rgb':\n",
    "            if len(args.std) != 3:\n",
    "                raise ValueError(\"When training with rgb, dim of std must be three.\")\n",
    "        elif args.modality == 'flow':\n",
    "            if len(args.std) != 1:\n",
    "                raise ValueError(\"When training with flow, dim of std must be three.\")\n",
    "        std = args.std\n",
    "\n",
    "    model = model.cuda(args.gpu)\n",
    "    model.eval()\n",
    "\n",
    "    if args.show_model:\n",
    "        if args.rank == 0:\n",
    "            print(model)\n",
    "        return 0\n",
    "\n",
    "    if args.pretrained is not None:\n",
    "        if args.rank == 0:\n",
    "            print(\"=> using pre-trained model '{}'\".format(arch_name))\n",
    "        checkpoint = torch.load(args.pretrained, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        del checkpoint  # dereference seems crucial\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        if args.rank == 0:\n",
    "            print(\"=> creating model '{}'\".format(arch_name))\n",
    "\n",
    "    if args.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if args.gpu is not None:\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            model.cuda(args.gpu)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            # the batch size should be divided by number of nodes as well\n",
    "            args.batch_size = int(args.batch_size / args.world_size)\n",
    "            args.workers = int(args.workers / ngpus_per_node)\n",
    "\n",
    "            if args.sync_bn:\n",
    "                process_group = torch.distributed.new_group(list(range(args.world_size)))\n",
    "                model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model, process_group)\n",
    "\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif args.gpu is not None:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model = model.cuda(args.gpu)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        # assign rank to 0\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        args.rank = 0\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    train_criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
    "    val_criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
    "\n",
    "    # Data loading code\n",
    "    val_list = os.path.join(args.datadir, val_list_name)\n",
    "\n",
    "    val_augmentor = get_augmentor(False, args.input_size, scale_range=args.scale_range, mean=mean,\n",
    "                                  std=std, disable_scaleup=args.disable_scaleup,\n",
    "                                  threed_data=args.threed_data,\n",
    "                                  is_flow=True if args.modality == 'flow' else False,\n",
    "                                  version=args.augmentor_ver)\n",
    "\n",
    "    val_dataset = VideoDataSet(args.datadir, val_list, args.groups, args.frames_per_group,\n",
    "                               num_clips=args.num_clips,\n",
    "                               modality=args.modality, image_tmpl=image_tmpl,\n",
    "                               dense_sampling=args.dense_sampling,\n",
    "                               transform=val_augmentor, is_train=False, test_mode=False,\n",
    "                               seperator=filename_seperator, filter_video=filter_video)\n",
    "\n",
    "    val_loader = build_dataflow(val_dataset, is_train=False, batch_size=args.batch_size,\n",
    "                                workers=args.workers,\n",
    "                                is_distributed=args.distributed)\n",
    "\n",
    "    log_folder = os.path.join(args.logdir, arch_name)\n",
    "    if args.rank == 0:\n",
    "        if not os.path.exists(log_folder):\n",
    "            os.makedirs(log_folder)\n",
    "\n",
    "    if args.evaluate:\n",
    "        val_top1, val_top5, val_losses, val_speed = validate(val_loader, model, val_criterion,\n",
    "                                                             gpu_id=args.gpu)\n",
    "        if args.rank == 0:\n",
    "            logfile = open(os.path.join(log_folder, 'evaluate_log.log'), 'a')\n",
    "            print(\n",
    "                'Val@{}: \\tLoss: {:4.4f}\\tTop@1: {:.4f}\\tTop@5: {:.4f}\\tSpeed: {:.2f} ms/batch'.format(\n",
    "                    args.input_size, val_losses, val_top1, val_top5, val_speed * 1000.0),\n",
    "                flush=True)\n",
    "            print(\n",
    "                'Val@{}: \\tLoss: {:4.4f}\\tTop@1: {:.4f}\\tTop@5: {:.4f}\\tSpeed: {:.2f} ms/batch'.format(\n",
    "                    args.input_size, val_losses, val_top1, val_top5, val_speed * 1000.0),\n",
    "                flush=True,\n",
    "                file=logfile)\n",
    "        return\n",
    "\n",
    "    train_list = os.path.join(args.datadir, train_list_name)\n",
    "\n",
    "    train_augmentor = get_augmentor(True, args.input_size, scale_range=args.scale_range, mean=mean,\n",
    "                                    std=std,\n",
    "                                    disable_scaleup=args.disable_scaleup,\n",
    "                                    threed_data=args.threed_data,\n",
    "                                    is_flow=True if args.modality == 'flow' else False,\n",
    "                                    version=args.augmentor_ver)\n",
    "\n",
    "    train_dataset = VideoDataSet(args.datadir, train_list, args.groups, args.frames_per_group,\n",
    "                                 num_clips=args.num_clips,\n",
    "                                 modality=args.modality, image_tmpl=image_tmpl,\n",
    "                                 dense_sampling=args.dense_sampling,\n",
    "                                 transform=train_augmentor, is_train=True, test_mode=False,\n",
    "                                 seperator=filename_seperator, filter_video=filter_video)\n",
    "\n",
    "    train_loader = build_dataflow(train_dataset, is_train=True, batch_size=args.batch_size,\n",
    "                                  workers=args.workers, is_distributed=args.distributed)\n",
    "\n",
    "    sgd_polices = model.parameters()\n",
    "    optimizer = torch.optim.SGD(sgd_polices, args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay,\n",
    "                                nesterov=args.nesterov)\n",
    "\n",
    "    if args.lr_scheduler == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, args.lr_steps[0], gamma=0.1)\n",
    "    elif args.lr_scheduler == 'multisteps':\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, args.lr_steps, gamma=0.1)\n",
    "    elif args.lr_scheduler == 'cosine':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, eta_min=0)\n",
    "    elif args.lr_scheduler == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True)\n",
    "\n",
    "    best_top1 = 0.0\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if args.rank == 0:\n",
    "            logfile = open(os.path.join(log_folder, 'log.log'), 'a')\n",
    "        if os.path.isfile(args.resume):\n",
    "            if args.rank == 0:\n",
    "                print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            if args.gpu is None:\n",
    "                checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "            else:\n",
    "                checkpoint = torch.load(args.resume, map_location='cuda:{}'.format(args.gpu))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_top1 = checkpoint['best_top1']\n",
    "            if args.gpu is not None:\n",
    "                if not isinstance(best_top1, float):\n",
    "                    best_top1 = best_top1.to(args.gpu)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            try:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            except:\n",
    "                pass\n",
    "            if args.rank == 0:\n",
    "                print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                      .format(args.resume, checkpoint['epoch']))\n",
    "            del checkpoint  # dereference seems crucial\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise ValueError(\"Checkpoint is not found: {}\".format(args.resume))\n",
    "    else:\n",
    "        if os.path.exists(os.path.join(log_folder, 'log.log')) and args.rank == 0:\n",
    "            shutil.copyfile(os.path.join(log_folder, 'log.log'), os.path.join(\n",
    "                log_folder, 'log.log.{}'.format(int(time.time()))))\n",
    "        if args.rank == 0:\n",
    "            logfile = open(os.path.join(log_folder, 'log.log'), 'w')\n",
    "\n",
    "    if args.rank == 0:\n",
    "        command = \" \".join(sys.argv)\n",
    "        tensorboard_logger.configure(os.path.join(log_folder))\n",
    "        print(command, flush=True)\n",
    "        print(args, flush=True)\n",
    "        print(model, flush=True)\n",
    "        print(command, file=logfile, flush=True)\n",
    "        print(args, file=logfile, flush=True)\n",
    "\n",
    "    if args.resume == '' and args.rank == 0:\n",
    "        print(model, file=logfile, flush=True)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        # train for one epoch\n",
    "        train_top1, train_top5, train_losses, train_speed, speed_data_loader, train_steps = \\\n",
    "            train(train_loader, model, train_criterion, optimizer, epoch + 1,\n",
    "                  display=args.print_freq, clip_gradient=args.clip_gradient,\n",
    "                  gpu_id=args.gpu, rank=args.rank)\n",
    "        if args.distributed:\n",
    "            dist.barrier()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_top1, val_top5, val_losses, val_speed = validate(val_loader, model, val_criterion,\n",
    "                                                             gpu_id=args.gpu)\n",
    "\n",
    "        # update current learning rate\n",
    "        if args.lr_scheduler == 'plateau':\n",
    "            scheduler.step(val_losses)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        if args.distributed:\n",
    "            dist.barrier()\n",
    "\n",
    "        # only logging at rank 0\n",
    "        if args.rank == 0:\n",
    "            print('Train: [{:03d}/{:03d}]\\tLoss: {:4.4f}\\tTop@1: {:.4f}\\tTop@5: {:.4f}\\t'\n",
    "                  'Speed: {:.2f} ms/batch\\tData loading: {:.2f} ms/batch'.format(\n",
    "                epoch + 1, args.epochs, train_losses, train_top1, train_top5, train_speed * 1000.0,\n",
    "                speed_data_loader * 1000.0), file=logfile, flush=True)\n",
    "            print('Train: [{:03d}/{:03d}]\\tLoss: {:4.4f}\\tTop@1: {:.4f}\\tTop@5: {:.4f}\\t'\n",
    "                  'Speed: {:.2f} ms/batch\\tData loading: {:.2f} ms/batch'.format(\n",
    "                epoch + 1, args.epochs, train_losses, train_top1, train_top5, train_speed * 1000.0,\n",
    "                speed_data_loader * 1000.0), flush=True)\n",
    "            print('Val  : [{:03d}/{:03d}]\\tLoss: {:4.4f}\\tTop@1: {:.4f}\\tTop@5: {:.4f}\\t'\n",
    "                  'Speed: {:.2f} ms/batch'.format(epoch + 1, args.epochs, val_losses, val_top1,\n",
    "                                                  val_top5, val_speed * 1000.0), file=logfile,\n",
    "                  flush=True)\n",
    "            print('Val  : [{:03d}/{:03d}]\\tLoss: {:4.4f}\\tTop@1: {:.4f}\\tTop@5: {:.4f}\\t'\n",
    "                  'Speed: {:.2f} ms/batch'.format(epoch + 1, args.epochs, val_losses, val_top1,\n",
    "                                                  val_top5, val_speed * 1000.0), flush=True)\n",
    "\n",
    "            # remember best prec@1 and save checkpoint\n",
    "            is_best = val_top1 > best_top1\n",
    "            best_top1 = max(val_top1, best_top1)\n",
    "\n",
    "            save_dict = {'epoch': epoch + 1,\n",
    "                         'arch': arch_name,\n",
    "                         'state_dict': model.state_dict(),\n",
    "                         'best_top1': best_top1,\n",
    "                         'optimizer': optimizer.state_dict(),\n",
    "                         'scheduler': scheduler.state_dict()\n",
    "                         }\n",
    "\n",
    "            save_checkpoint(save_dict, is_best, filepath=log_folder)\n",
    "            try:\n",
    "                # get_lr get all lrs for every layer of current epoch, assume the lr for all layers are identical\n",
    "                lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            except Exception as e:\n",
    "                lr = None\n",
    "            if lr is not None:\n",
    "                tensorboard_logger.log_value('learning-rate', lr, epoch + 1)\n",
    "            tensorboard_logger.log_value('val-top1', val_top1, epoch + 1)\n",
    "            tensorboard_logger.log_value('val-loss', val_losses, epoch + 1)\n",
    "            tensorboard_logger.log_value('train-top1', train_top1, epoch + 1)\n",
    "            tensorboard_logger.log_value('train-loss', train_losses, epoch + 1)\n",
    "            tensorboard_logger.log_value('best-val-top1', best_top1, epoch + 1)\n",
    "\n",
    "        if args.distributed:\n",
    "            dist.barrier()\n",
    "\n",
    "    if args.rank == 0:\n",
    "        logfile.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
