{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2709ebac",
   "metadata": {},
   "source": [
    "\n",
    "# Two-Stream Action Recognition (RGB + Optical Flow) on UCF50 (PyTorch)\n",
    "\n",
    "Chương trình nguồn này huấn luyện một mô hình Two-Stream cho action recognition với:\n",
    "\n",
    "***RGB stream***: các image frames được lấy mẫu từ videos\n",
    "\n",
    "***Optical-Flow stream***: các stacks của trường dòng $(u,v)$ được tính giữa các consecutive frames (on the fly)\n",
    "\n",
    "Code hỗ trợ folder-based datasets như UCF50, thực hiện **train/validation** split từ class folders, ghi lại metrics, lưu checkpoints và vẽ các đường cong **loss/accuracy**.\n",
    "\n",
    "**Note**: **Optical flow** được tính bằng OpenCV (TV-L1 hoặc Farneback). Việc tính flow on the fly tốn nhiều tài nguyên CPU; bạn có thể giảm num_segments, clip_len_rgb, và flow_stack để thử nghiệm nhanh hơn hoặc **precompute/cached flows**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09267e90-150a-429c-bce4-5420415d6dc5",
   "metadata": {},
   "source": [
    "### Gọi thư viện\n",
    "Mục đích: Nạp thư viện (PyTorch, Torchvision, OpenCV, NumPy, Matplotlib, sklearn), kiểm tra phiên bản, chọn device.\n",
    "Đầu vào: Môi trường Python đã cài các gói cần thiết.\n",
    "Đầu ra: In ra phiên bản Torch/Torchvision/OpenCV; biến device (CUDA nếu có GPU).\n",
    "Công cụ/Xử lý chính:\n",
    "\n",
    "import torch, torchvision, cv2, numpy, matplotlib, sklearn.metrics.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a89a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.6.0+cpu\n",
      "Torchvision: 0.21.0+cpu\n",
      "OpenCV: 4.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#@title Environment & Imports\n",
    "import os, sys, json, time, random, math, shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"OpenCV:\", cv2.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db2a3e4-855b-4e9f-80a4-a6c94f383824",
   "metadata": {},
   "source": [
    "### Cấu hình  - Đặt lại đường dẫn phù hợp \n",
    "\n",
    "**Mục đích**: Khai báo cấu hình huấn luyện bằng @dataclass và lưu config.json.\n",
    "**Đầu vào**: Không (giá trị mặc định có thể sửa trực tiếp).\n",
    "**Đầu ra**: File config.json trong output_dir; in cấu hình.\n",
    "***Công cụ/Xử lý chính:***\n",
    "**Config**: đường dẫn data_root, output_dir; tham số data (train_ratio, num_segments, clip_len_rgb, flow_stack, frame_size, flow_method), train (batch_size, epochs, lr, weight_decay), mô hình (rgb_backbone, flow_backbone, fusion, trọng số fusion), logging, v.v.\n",
    "*Tạo thư mục output* và ghi JSON.\n",
    "\n",
    "**Tham số quan trọng**:\n",
    "\n",
    "*data_root*: thư mục UCF50 (mỗi class là 1 thư mục).\n",
    "\n",
    "*num_segments, flow_stack, frame_size, flow_method.*\n",
    "\n",
    "**fusion**: \"late_sum\" (nhanh/đơn giản) hoặc \"concat_fc\" (học được trọng số)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26e6869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config saved to: D:/Teach_n_Train/Advanced_Lessons_CV/LABS/CV_3D_Data/two_stream_runs/ucf50_exp1\\config.json\n",
      "Config(data_root='D:/Teach_n_Train/Advanced_Lessons_CV/LABS/CV_3D_Data/UCF50', output_dir='D:/Teach_n_Train/Advanced_Lessons_CV/LABS/CV_3D_Data/two_stream_runs/ucf50_exp1', train_ratio=0.8, seed=42, num_segments=3, clip_len_rgb=1, flow_stack=10, frame_size=224, flow_method='tvl1', batch_size=8, num_workers=4, epochs=10, lr=0.001, weight_decay=0.0001, rgb_backbone='resnet18', flow_backbone='resnet18', fusion='late_sum', fusion_weight_rgb=0.5, fusion_weight_flow=0.5, save_every=1, log_interval=20, compute_confusion=True)\n"
     ]
    }
   ],
   "source": [
    "#@title Configuration\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Paths\n",
    "    data_root: str = \"D:/Teach_n_Train/Advanced_Lessons_CV/LABS/CV_3D_Data/UCF50\"  #@param {type:\"string\"}\n",
    "    output_dir: str = \"D:/Teach_n_Train/Advanced_Lessons_CV/LABS/CV_3D_Data/two_stream_runs/ucf50_exp1\"  #@param {type:\"string\"}\n",
    "\n",
    "    # Data settings\n",
    "    train_ratio: float = 0.8  #@param {type:\"number\"}\n",
    "    seed: int = 42  #@param {type:\"number\"}\n",
    "    # Video sampling\n",
    "    num_segments: int = 3  # number of temporal segments per video  #@param {type:\"integer\"}\n",
    "    clip_len_rgb: int = 1  # frames per RGB segment (use 1 for TSN-like)  #@param {type:\"integer\"}\n",
    "    flow_stack: int = 10    # number of consecutive flows (= 2*(flow_stack) channels)  #@param {type:\"integer\"}\n",
    "    frame_size: int = 224   # resize short side to this (keeping aspect) then center crop 224x224  #@param {type:\"integer\"}\n",
    "    # Flow method: 'tvl1' or 'farneback'\n",
    "    flow_method: str = \"tvl1\"  #@param [\"tvl1\",\"farneback\"]\n",
    "    # Train\n",
    "    batch_size: int = 8  #@param {type:\"integer\"}\n",
    "    num_workers: int = 4  #@param {type:\"integer\"}\n",
    "    epochs: int = 10  #@param {type:\"integer\"}\n",
    "    lr: float = 1e-3  #@param {type:\"number\"}\n",
    "    weight_decay: float = 1e-4  #@param {type:\"number\"}\n",
    "    # Model\n",
    "    rgb_backbone: str = \"resnet18\"  #@param [\"resnet18\",\"resnet34\",\"resnet50\"]\n",
    "    flow_backbone: str = \"resnet18\"  #@param [\"resnet18\",\"resnet34\",\"resnet50\"]\n",
    "    fusion: str = \"late_sum\"  #@param [\"late_sum\",\"concat_fc\"]\n",
    "    fusion_weight_rgb: float = 0.5  # weight for RGB logits in late_sum  #@param {type:\"number\"}\n",
    "    fusion_weight_flow: float = 0.5  # weight for Flow logits in late_sum  #@param {type:\"number\"}\n",
    "    # Checkpointing & logging\n",
    "    save_every: int = 1  # save per epoch  #@param {type:\"integer\"}\n",
    "    log_interval: int = 20  # batches  #@param {type:\"integer\"}\n",
    "    # Evaluation\n",
    "    compute_confusion: bool = True  #@param {type:\"boolean\"}\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Prepare output dirs\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "with open(os.path.join(cfg.output_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(asdict(cfg), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Config saved to:\", os.path.join(cfg.output_dir, \"config.json\"))\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07124ba1-e915-43a2-b704-3bfcb921f9c0",
   "metadata": {},
   "source": [
    "### Data Utilities: listing, splitting, frame/flow sampling\n",
    "\n",
    "**Mục đích**: Các tiện ích dữ liệu: liệt kê video theo lớp, tách train/val, đọc toàn bộ khung hình, resize+center-crop, tính flow stack quanh khung tâm (on-the-fly).\n",
    "\n",
    "**Đầu vào**: \n",
    "root dữ liệu; video_path.\n",
    "Số segment, độ dài clip, phương pháp flow.\n",
    "\n",
    "**Đầu ra**:\n",
    "class_to_files (dict lớp → danh sách video).\n",
    "splits (list (path,label) cho train/val).\n",
    "frames đã đọc; ảnh đã resize+crop; uv_stack (H×W×(2*stack)).\n",
    "\n",
    "**Công cụ/Xử lý chính**:\n",
    "*cv2.VideoCapture* đọc video sang RGB; resize theo “short side = frame_size”, center-crop về frame_size×frame_size.\n",
    "*sample_indices*: chia đều mốc thời gian kiểu TSN.\n",
    "*compute_flow_stack*:\n",
    "TV-L1 (cv2.optflow.DualTVL1OpticalFlow_create) hoặc Farneback (cv2.calcOpticalFlowFarneback).\n",
    "Ghép stack cặp (u,v) liên tiếp xung quanh frame tâm → (H, W, 2*stack); chuẩn hoá bằng clip theo 95th percentile rồi chia để ~[-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a09e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Data Utilities: listing, splitting, frame/flow sampling\n",
    "\n",
    "def list_video_files(root: str, exts=(\".avi\", \".mp4\", \".mov\", \".mkv\")) -> Dict[str, List[str]]:\n",
    "    root_p = Path(root)\n",
    "    class_to_files = {}\n",
    "    for cls_dir in sorted([p for p in root_p.iterdir() if p.is_dir()]):\n",
    "        files = []\n",
    "        for ext in exts:\n",
    "            files.extend([str(p) for p in cls_dir.rglob(f\"*{ext}\")])\n",
    "        if files:\n",
    "            class_to_files[cls_dir.name] = sorted(files)\n",
    "    return class_to_files\n",
    "\n",
    "def split_train_val(class_to_files: Dict[str, List[str]], train_ratio: float, seed: int=42):\n",
    "    rng = random.Random(seed)\n",
    "    splits = {\"train\": [], \"val\": []}\n",
    "    class_names = sorted(class_to_files.keys())\n",
    "    for ci, cls in enumerate(class_names):\n",
    "        files = class_to_files[cls][:]\n",
    "        rng.shuffle(files)\n",
    "        k = int(len(files) * train_ratio)\n",
    "        train_files = files[:k]\n",
    "        val_files = files[k:]\n",
    "        for fp in train_files:\n",
    "            splits[\"train\"].append((fp, ci))\n",
    "        for fp in val_files:\n",
    "            splits[\"val\"].append((fp, ci))\n",
    "    return splits, class_names\n",
    "\n",
    "def sample_indices(num_frames: int, num_segments: int, clip_len: int=1):\n",
    "    \"\"\"Uniformly sample indices for TSN-style segments.\"\"\"\n",
    "    # Divide [0, num_frames-1] into num_segments intervals and pick center indexes\n",
    "    segment_length = num_frames / float(num_segments)\n",
    "    inds = []\n",
    "    for s in range(num_segments):\n",
    "        start = int(round(segment_length * s))\n",
    "        end = int(round(segment_length * (s+1))) - 1\n",
    "        if end < start:\n",
    "            end = start\n",
    "        center = (start + end) // 2\n",
    "        # For clip_len>1, create a small window around center\n",
    "        half = clip_len // 2\n",
    "        seg_inds = [min(max(center + i, 0), num_frames-1) for i in range(-half, -half + clip_len)]\n",
    "        inds.append(seg_inds)\n",
    "    return inds  # list of [clip_len] each\n",
    "\n",
    "def read_all_frames_cv(video_path: str) -> List[np.ndarray]:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Failed to open video: {video_path}\")\n",
    "    frames = []\n",
    "    ok, frame = cap.read()\n",
    "    while ok:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "        ok, frame = cap.read()\n",
    "    cap.release()\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(f\"No frames read from video: {video_path}\")\n",
    "    return frames\n",
    "\n",
    "def resize_and_center_crop(img: np.ndarray, size: int=224) -> np.ndarray:\n",
    "    h, w, _ = img.shape\n",
    "    # Resize short side to size\n",
    "    if h < w:\n",
    "        new_h, new_w = size, int(w * (size / h))\n",
    "    else:\n",
    "        new_w, new_h = size, int(h * (size / w))\n",
    "    img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    # Center crop\n",
    "    h, w, _ = img.shape\n",
    "    top = (h - size) // 2\n",
    "    left = (w - size) // 2\n",
    "    img = img[top:top+size, left:left+size, :]\n",
    "    return img\n",
    "\n",
    "def compute_flow_stack(frames: List[np.ndarray], center_index: int, stack: int=10, method=\"tvl1\") -> np.ndarray:\n",
    "    \"\"\"Compute a stack of (u,v) optical flows around center_index using consecutive frames.\n",
    "    Returns array shape (H, W, 2*stack), values scaled to [-1,1] approximately.\n",
    "    \"\"\"\n",
    "    h, w, _ = frames[0].shape\n",
    "    flows = []\n",
    "    # define range: e.g., center- stack to center-1\n",
    "    start = max(center_index - stack, 0)\n",
    "    end = min(center_index + stack, len(frames)-1)\n",
    "    # Construct pairs from [start..center-1] to [start+1..center]\n",
    "    # If not enough before, pad from after; keep exactly 'stack' pairs\n",
    "    pairs = []\n",
    "    # Try backward pairs ending at center\n",
    "    b_needed = stack\n",
    "    for i in range(center_index - b_needed, center_index):\n",
    "        a = max(0, i)\n",
    "        b = min(len(frames)-1, a+1)\n",
    "        pairs.append((a, b))\n",
    "    pairs = pairs[-stack:]\n",
    "    # If less than stack (start near 0), append forward pairs after center\n",
    "    while len(pairs) < stack:\n",
    "        i = min(len(frames)-2, pairs[-1][1] if pairs else center_index)\n",
    "        a = i\n",
    "        b = i+1\n",
    "        pairs.append((a, b))\n",
    "\n",
    "    # Compute flow for each pair\n",
    "    if method == \"tvl1\":\n",
    "        tvl1 = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "        for (i, j) in pairs:\n",
    "            prev = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
    "            nxt  = cv2.cvtColor(frames[j], cv2.COLOR_RGB2GRAY)\n",
    "            flow = tvl1.calc(prev, nxt, None)  # HxWx2 float32\n",
    "            flows.append(flow)\n",
    "    else:  # Farneback\n",
    "        for (i, j) in pairs:\n",
    "            prev = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
    "            nxt  = cv2.cvtColor(frames[j], cv2.COLOR_RGB2GRAY)\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev, nxt, None,\n",
    "                                                pyr_scale=0.5, levels=3, winsize=15,\n",
    "                                                iterations=3, poly_n=5, poly_sigma=1.1, flags=0)\n",
    "            # Farneback returns 2-channel flow\n",
    "            flows.append(flow.astype(np.float32))\n",
    "    # Stack along channel axis\n",
    "    uv_stack = np.concatenate([f for f in flows], axis=2)  # H x W x (2*stack)\n",
    "    # Normalize: clip large values and scale\n",
    "    mag = np.sqrt(np.sum(uv_stack**2, axis=2, keepdims=True))\n",
    "    m95 = np.percentile(mag, 95) + 1e-6\n",
    "    uv_stack = np.clip(uv_stack, -m95, m95) / m95  # roughly in [-1,1]\n",
    "    return uv_stack.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08543c7b-f182-4bbb-8233-94ca8c378414",
   "metadata": {},
   "source": [
    "## Dataset (TSN-style sampling for Two-Stream)\n",
    "\n",
    "**Mục đích**: Định nghĩa TwoStreamVideoDataset (version on-the-fly).\n",
    "**Đầu vào**: samples (list (video_path, label)), class_names, cfg.\n",
    "**Đầu ra**: Cho mỗi item:\n",
    "rgb_clip: tensor K×3×H×W.\n",
    "flow_clip: tensor K×(2*stack)×H×W.\n",
    "label (int).\n",
    "\n",
    "**Công cụ/Xử lý chính**:\n",
    "Đọc và preprocess tất cả frames.\n",
    "Lấy K frame tâm cho RGB; với mỗi tâm, tính uv_stack cho Flow.\n",
    "Chuẩn hoá RGB theo ImageNet mean/std.\n",
    "Ghép thành batch ở DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0433bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Dataset (TSN-style sampling for Two-Stream)\n",
    "\n",
    "class TwoStreamVideoDataset(Dataset):\n",
    "    def __init__(self, samples: List[Tuple[str,int]], class_names: List[str], cfg):\n",
    "        self.samples = samples\n",
    "        self.class_names = class_names\n",
    "        self.cfg = cfg\n",
    "        self.rgb_tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # For flow stream we keep as float32 already scaled to [-1,1]; then standardize lightly\n",
    "        self.flow_tf = transforms.Compose([\n",
    "            transforms.ToTensor(),  # HxWxC -> CxHxW\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.samples[idx]\n",
    "        frames_all = read_all_frames_cv(video_path)\n",
    "        # Resize+Center-crop all frames first for speed consistency\n",
    "        frames_all = [resize_and_center_crop(f, self.cfg.frame_size) for f in frames_all]\n",
    "        n = len(frames_all)\n",
    "\n",
    "        # Sample indices for RGB (TSN style)\n",
    "        seg_inds_rgb = sample_indices(n, self.cfg.num_segments, self.cfg.clip_len_rgb)  # list of lists\n",
    "        # Pick center frame for each segment (clip_len_rgb may be >1; we use the center item index)\n",
    "        rgb_indices = [inds[len(inds)//2] for inds in seg_inds_rgb]\n",
    "\n",
    "        # For Flow segments, compute flow stacks centered at same positions\n",
    "        flow_centers = rgb_indices  # align flow centers with RGB segments\n",
    "\n",
    "        # Build RGB tensor: stack K frames into Kx3xHxW\n",
    "        rgb_imgs = [frames_all[i] for i in rgb_indices]\n",
    "        rgb_tensors = [self.rgb_tf(img) for img in rgb_imgs]  # 3xHxW\n",
    "        rgb_clip = torch.stack(rgb_tensors, dim=0)  # Kx3xHxW\n",
    "\n",
    "        # Build Flow tensor: for each center, compute uv_stack => C=2*flow_stack\n",
    "        flow_stacks = []\n",
    "        for c in flow_centers:\n",
    "            uv = compute_flow_stack(frames_all, c, stack=self.cfg.flow_stack, method=self.cfg.flow_method)\n",
    "            flow_stacks.append(self.flow_tf(uv))  # CxHxW\n",
    "        flow_clip = torch.stack(flow_stacks, dim=0)  # Kx(2*stack)xHxW\n",
    "\n",
    "        return rgb_clip, flow_clip, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4f28a-3725-4028-8952-0e197aa85db9",
   "metadata": {},
   "source": [
    "## Two-Stream Model Definition\n",
    "\n",
    "**Mục đích:** Định nghĩa backbone ResNet cho RGB và Flow; module Two-Stream; các kiểu fusion.\n",
    "**Đầu vào**: num_classes, cfg.\n",
    "**Đầu ra**: TwoStreamModel (nn.Module).\n",
    "**Công cụ/Xử lý chính**:\n",
    "\n",
    "*make_resnet_backbone(name, in_channels)*:\n",
    "Tải weights ImageNet cho RGB (3 kênh).\n",
    "Thay conv1 cho Flow (số kênh = 2*flow_stack), khởi tạo lặp/scale từ kernel 3 kênh.\n",
    "Bỏ fc gốc → Identity, trả feat_dim.\n",
    "\n",
    "*TwoStreamModel.forward*:\n",
    "Gộp B×K×C×H×W → tính đặc trưng cho từng frame → trung bình theo K (temporal average).\n",
    "\n",
    "*Fusion*:\n",
    "concat_fc: concat (RGB|Flow) → FC.\n",
    "late_sum: hai head FC riêng, tổng có trọng số theo fusion_weight_rgb/flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569b2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Two-Stream Model Definition\n",
    "\n",
    "def make_resnet_backbone(name: str, in_channels: int=3, pretrained: bool=True):\n",
    "    if name == \"resnet18\":\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained and in_channels==3 else None)\n",
    "    elif name == \"resnet34\":\n",
    "        m = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained and in_channels==3 else None)\n",
    "    elif name == \"resnet50\":\n",
    "        m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained and in_channels==3 else None)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone: \" + name)\n",
    "    # Adjust first conv if in_channels != 3\n",
    "    if in_channels != 3:\n",
    "        w = m.conv1.weight.data\n",
    "        m.conv1 = nn.Conv2d(in_channels, m.conv1.out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        if w.shape[1] == 3:\n",
    "            with torch.no_grad():\n",
    "                # repeat/average to new in_channels\n",
    "                repeat = int(math.ceil(in_channels / 3))\n",
    "                w_rep = w.repeat(1, repeat, 1, 1)[:, :in_channels, :, :]\n",
    "                w_new = w_rep * (3.0 / in_channels)  # preserve activation scale roughly\n",
    "                m.conv1.weight.copy_(w_new)\n",
    "    # Replace classifier with identity; return feature_dim\n",
    "    feat_dim = m.fc.in_features\n",
    "    m.fc = nn.Identity()\n",
    "    return m, feat_dim\n",
    "\n",
    "class TwoStreamModel(nn.Module):\n",
    "    def __init__(self, num_classes: int, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.rgb_backbone, rgb_dim = make_resnet_backbone(cfg.rgb_backbone, in_channels=3, pretrained=True)\n",
    "        flow_in_ch = 2 * cfg.flow_stack  # 2 per flow (u,v) times stack\n",
    "        self.flow_backbone, flow_dim = make_resnet_backbone(cfg.flow_backbone, in_channels=flow_in_ch, pretrained=False)\n",
    "\n",
    "        if cfg.fusion == \"concat_fc\":\n",
    "            self.classifier = nn.Linear(rgb_dim + flow_dim, num_classes)\n",
    "        else:\n",
    "            self.rgb_fc = nn.Linear(rgb_dim, num_classes)\n",
    "            self.flow_fc = nn.Linear(flow_dim, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_stream(self, backbone, x_clip):\n",
    "        # x_clip: B x K x C x H x W (we will merge BK then average logits across K)\n",
    "        B, K, C, H, W = x_clip.shape\n",
    "        x = x_clip.view(B*K, C, H, W)\n",
    "        feats = backbone(x)  # (B*K) x D\n",
    "        feats = feats.view(B, K, -1)  # B x K x D\n",
    "        feats = feats.mean(dim=1)     # average over temporal segments\n",
    "        return feats  # B x D\n",
    "\n",
    "    def forward(self, rgb_clip, flow_clip):\n",
    "        rgb_feats = self.forward_stream(self.rgb_backbone, rgb_clip)\n",
    "        flow_feats = self.forward_stream(self.flow_backbone, flow_clip)\n",
    "\n",
    "        if self.cfg.fusion == \"concat_fc\":\n",
    "            feats = torch.cat([rgb_feats, flow_feats], dim=1)\n",
    "            logits = self.classifier(feats)\n",
    "        else:  # late_sum\n",
    "            rgb_logits = self.rgb_fc(rgb_feats)\n",
    "            flow_logits = self.flow_fc(flow_feats)\n",
    "            logits = self.cfg.fusion_weight_rgb * rgb_logits + self.cfg.fusion_weight_flow * flow_logits\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c816158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Training / Evaluation Helpers\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def accuracy_top1(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, epoch, cfg):\n",
    "    model.train()\n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "    start = time.time()\n",
    "    for i, (rgb, flow, y) in enumerate(loader, 1):\n",
    "        rgb = rgb.to(device, non_blocking=True)\n",
    "        flow = flow.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(rgb, flow)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = accuracy_top1(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc\n",
    "\n",
    "        if i % cfg.log_interval == 0:\n",
    "            print(f\"[Epoch {epoch}] Step {i}/{len(loader)}  loss={running_loss/i:.4f}  acc={running_acc/i:.4f}  time={(time.time()-start):.1f}s\")\n",
    "\n",
    "    return running_loss/len(loader), running_acc/len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    eloss, eacc = 0.0, 0.0\n",
    "    all_y, all_p = [], []\n",
    "    for rgb, flow, y in loader:\n",
    "        rgb = rgb.to(device, non_blocking=True)\n",
    "        flow = flow.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(rgb, flow)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = accuracy_top1(logits, y)\n",
    "        eloss += loss.item()\n",
    "        eacc += acc\n",
    "        all_y.append(y.cpu().numpy())\n",
    "        all_p.append(logits.argmax(dim=1).cpu().numpy())\n",
    "    all_y = np.concatenate(all_y) if all_y else np.array([])\n",
    "    all_p = np.concatenate(all_p) if all_p else np.array([])\n",
    "    return eloss/len(loader), eacc/len(loader), all_y, all_p\n",
    "\n",
    "def save_ckpt(model, optimizer, epoch, cfg, class_names):\n",
    "    path = os.path.join(cfg.output_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    payload = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"cfg\": asdict(cfg),\n",
    "        \"class_names\": class_names,\n",
    "    }\n",
    "    torch.save(payload, path)\n",
    "    print(\"Saved checkpoint:\", path)\n",
    "    return path\n",
    "\n",
    "def plot_curves(history, out_dir):\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"Loss\"); plt.legend(); plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    loss_path = os.path.join(out_dir, \"loss_curve.png\")\n",
    "    plt.savefig(loss_path, dpi=150); plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.title(\"Accuracy\"); plt.legend(); plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    acc_path = os.path.join(out_dir, \"acc_curve.png\")\n",
    "    plt.savefig(acc_path, dpi=150); plt.show()\n",
    "\n",
    "    return loss_path, acc_path\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix'):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-8)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title); plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=7)\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba015059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classes: ['BaseballPitch', 'Basketball', 'BenchPress', 'Biking', 'Billiards'] ... (total: 50 )\n",
      "Num classes: 50 | Train videos: 5326 | Val videos: 1355\n",
      "Class mapping saved to class_to_idx.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Build Datasets and DataLoaders\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "class_to_files = list_video_files(cfg.data_root)\n",
    "if not class_to_files:\n",
    "    print(\"No class folders / videos were found. Please check `cfg.data_root`.\")\n",
    "else:\n",
    "    print(\"Found classes:\", list(class_to_files.keys())[:5], \"... (total:\", len(class_to_files), \")\")\n",
    "\n",
    "splits, class_names = split_train_val(class_to_files, cfg.train_ratio, cfg.seed)\n",
    "num_classes = len(class_names)\n",
    "print(\"Num classes:\", num_classes, \"| Train videos:\", len(splits[\"train\"]), \"| Val videos:\", len(splits[\"val\"]))\n",
    "\n",
    "train_ds = TwoStreamVideoDataset(splits[\"train\"], class_names, cfg)\n",
    "val_ds   = TwoStreamVideoDataset(splits[\"val\"],   class_names, cfg)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(class_names)}\n",
    "with open(os.path.join(cfg.output_dir, \"class_to_idx.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(class_to_idx, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Class mapping saved to class_to_idx.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c8ee3-1573-43e0-ace8-53fda97d4868",
   "metadata": {},
   "source": [
    "#### Tải weight của resnet18 để dựng mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2de2b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\WinIF Chung/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:01<00:00, 35.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 22,457,636  |  Trainable: 22,457,636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Initialize Model & Optimizer\n",
    "\n",
    "model = TwoStreamModel(num_classes=num_classes, cfg=cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {total_params:,}  |  Trainable: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Train\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "best_val = -1.0\n",
    "metrics_log = []\n",
    "\n",
    "for epoch in range(1, cfg.epochs+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, epoch, cfg)\n",
    "    va_loss, va_acc, y_true, y_pred = evaluate(model, val_loader)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss)\n",
    "    history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    ep_metrics = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": tr_loss,\n",
    "        \"train_acc\": tr_acc,\n",
    "        \"val_loss\": va_loss,\n",
    "        \"val_acc\": va_acc\n",
    "    }\n",
    "    metrics_log.append(ep_metrics)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}: train_loss={tr_loss:.4f}, train_acc={tr_acc:.4f} | val_loss={va_loss:.4f}, val_acc={va_acc:.4f}\")\n",
    "\n",
    "    if epoch % cfg.save_every == 0:\n",
    "        save_ckpt(model, optimizer, epoch, cfg, class_names)\n",
    "\n",
    "    # Track best\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        best_path = os.path.join(cfg.output_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"cfg\": asdict(cfg),\n",
    "            \"class_names\": class_names\n",
    "        }, best_path)\n",
    "        print(\"Saved best model to:\", best_path)\n",
    "\n",
    "# Save history & metrics\n",
    "with open(os.path.join(cfg.output_dir, \"history.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "with open(os.path.join(cfg.output_dir, \"metrics_log.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_log, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Training complete. Logs saved in\", cfg.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f25a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Plot Curves & Confusion Matrix (optional)\n",
    "\n",
    "loss_path, acc_path = plot_curves(history, cfg.output_dir)\n",
    "print(\"Saved plots to:\", loss_path, \"and\", acc_path)\n",
    "\n",
    "if cfg.compute_confusion and len(val_ds) > 0:\n",
    "    # Recompute predictions on val set for CM\n",
    "    _, _, y_true, y_pred = evaluate(model, val_loader)\n",
    "    if len(y_true) > 0:\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "        plot_confusion_matrix(cm, classes=class_names, normalize=True, title=\"Normalized Confusion Matrix\")\n",
    "        cm_path = os.path.join(cfg.output_dir, \"confusion_matrix.png\")\n",
    "        plt.savefig(cm_path, dpi=150); plt.show()\n",
    "        print(\"Saved confusion matrix to:\", cm_path)\n",
    "\n",
    "        # Also save text report\n",
    "        report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
    "        rep_path = os.path.join(cfg.output_dir, \"classification_report.txt\")\n",
    "        with open(rep_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report)\n",
    "        print(\"Saved classification report to:\", rep_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Inference on a Single Video\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_video(model, video_path: str, class_names: List[str], cfg):\n",
    "    model.eval()\n",
    "    frames_all = read_all_frames_cv(video_path)\n",
    "    frames_all = [resize_and_center_crop(f, cfg.frame_size) for f in frames_all]\n",
    "    n = len(frames_all)\n",
    "    seg_inds_rgb = sample_indices(n, cfg.num_segments, cfg.clip_len_rgb)\n",
    "    rgb_indices = [inds[len(inds)//2] for inds in seg_inds_rgb]\n",
    "    flow_centers = rgb_indices\n",
    "\n",
    "    rgb_imgs = [frames_all[i] for i in rgb_indices]\n",
    "    rgb_tensors = [transforms.ToTensor()(img) for img in rgb_imgs]\n",
    "    rgb_tensors = [transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(t) for t in rgb_tensors]\n",
    "    rgb_clip = torch.stack(rgb_tensors, dim=0).unsqueeze(0).to(device)  # 1xKx3xHxW\n",
    "\n",
    "    flow_stacks = []\n",
    "    for c in flow_centers:\n",
    "        uv = compute_flow_stack(frames_all, c, stack=cfg.flow_stack, method=cfg.flow_method)\n",
    "        flow_stacks.append(transforms.ToTensor()(uv))\n",
    "    flow_clip = torch.stack(flow_stacks, dim=0).unsqueeze(0).to(device)  # 1xKxCxHxW\n",
    "\n",
    "    logits = model(rgb_clip, flow_clip)\n",
    "    probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    topk = probs.argsort()[::-1][:5]\n",
    "    return [(class_names[i], float(probs[i])) for i in topk]\n",
    "\n",
    "# Example (update the path)\n",
    "# demo_path = \"/path/to/UCF50/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\"\n",
    "# print(predict_video(model, demo_path, class_names, cfg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44395e99",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Tips\n",
    "\n",
    "- **Data layout** đề xuất:\n",
    "  ```\n",
    "  UCF50/\n",
    "    ClassA/\n",
    "      video1.avi\n",
    "      video2.avi\n",
    "      ...\n",
    "    ClassB/\n",
    "      v_*.avi\n",
    "      ...\n",
    "    ...\n",
    "  ```\n",
    "- **Speed**: Việc tính TV-L1 on the fly khá nặng. Hãy thử:\n",
    "  - Giảm `num_segments` (e.g., 2)\n",
    "  - Giảm `flow_stack` (e.g., 5)\n",
    "  - Đổi `flow_method` thành `\"farneback\"`\n",
    "  - Precompute và cache flows (chỉnh Dataset để load từ file .npy)..\n",
    "- **Memory**: Dùng `batch_size` nhỏ hơn nếu gặp lỗi OOM.\n",
    "- **Backbones**: Có thể đổi thành `resnet50` để có accuracy tốt hơn (nhưng sẽ chậm hơn).\n",
    "- **Fusion**: Thử dùng `concat_fc` để kết hợp learnable (có thể chậm hơn).\n",
    "- **Checkpoints**: Trạng thái mô hình tốt nhất được lưu tại `best_model.pth`. Checkpoints theo epoch là `checkpoint_epoch_*.pth`.\n",
    "- **Reproducibility**:Kết quả có thể thay đổi tùy thuộc vào sự khác biệt trong video decoding và các tham số flow computation..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d7000",
   "metadata": {},
   "source": [
    "\n",
    "## Extensions: Flow Cache, LR Scheduler, Early Stopping, TorchScript Export\n",
    "Bên dưới chúng ta bổ sung:\n",
    "**Flow precompute & cache to disk** để tăng tốc quá trình training.\n",
    "**Learning rate scheduler** (CosineAnnealingLR / StepLR / ReduceLROnPlateau).\n",
    "**Early stopping** dựa trên **validation accuracy**.\n",
    "**Export**: lưu .pt (state dict) và TorchScript cho mục đích deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Extended Configuration (cache + LR scheduler + early stopping)\n",
    "from dataclasses import asdict\n",
    "\n",
    "# ---- Flow cache options ----\n",
    "use_flow_cache = True            #@param {type:\"boolean\"}\n",
    "flow_cache_dir = \"./two_stream_runs/flow_cache\"  #@param {type:\"string\"}\n",
    "precompute_flow_now = False      #@param {type:\"boolean\"}\n",
    "precompute_on_split = \"train+val\"  #@param [\"train\",\"val\",\"train+val\",\"all\"]\n",
    "cache_flow_method = \"tvl1\"       #@param [\"tvl1\",\"farneback\"]\n",
    "\n",
    "# ---- Scheduler options ----\n",
    "lr_scheduler_type = \"cosine\"     #@param [\"cosine\",\"step\",\"plateau\",\"none\"]\n",
    "step_size = 5                    #@param {type:\"integer\"}\n",
    "gamma = 0.1                      #@param {type:\"number\"}\n",
    "cosine_Tmax = 10                 # cycles over epochs  #@param {type:\"integer\"}\n",
    "\n",
    "# ---- Early stopping ----\n",
    "early_stop_on = \"val_acc\"        #@param [\"val_acc\",\"val_loss\"]\n",
    "early_stop_mode = \"max\"          #@param [\"max\",\"min\"]\n",
    "early_stop_patience = 5          #@param {type:\"integer\"}\n",
    "\n",
    "# ---- Export options ----\n",
    "export_dir = \"./two_stream_runs/exports\"  #@param {type:\"string\"}\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# attach to cfg safely (cfg is defined above)\n",
    "for k, v in dict(\n",
    "    use_flow_cache=use_flow_cache,\n",
    "    flow_cache_dir=flow_cache_dir,\n",
    "    precompute_flow_now=precompute_flow_now,\n",
    "    precompute_on_split=precompute_on_split,\n",
    "    cache_flow_method=cache_flow_method,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    step_size=step_size,\n",
    "    gamma=gamma,\n",
    "    cosine_Tmax=cosine_Tmax,\n",
    "    early_stop_on=early_stop_on,\n",
    "    early_stop_mode=early_stop_mode,\n",
    "    early_stop_patience=early_stop_patience,\n",
    "    export_dir=export_dir,\n",
    ").items():\n",
    "    setattr(cfg, k, v)\n",
    "\n",
    "# persist extra config\n",
    "extra_cfg_path = os.path.join(cfg.output_dir, \"config_extra.json\")\n",
    "with open(extra_cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"use_flow_cache\": use_flow_cache,\n",
    "        \"flow_cache_dir\": flow_cache_dir,\n",
    "        \"precompute_flow_now\": precompute_flow_now,\n",
    "        \"precompute_on_split\": precompute_on_split,\n",
    "        \"cache_flow_method\": cache_flow_method,\n",
    "        \"lr_scheduler_type\": lr_scheduler_type,\n",
    "        \"step_size\": step_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"cosine_Tmax\": cosine_Tmax,\n",
    "        \"early_stop_on\": early_stop_on,\n",
    "        \"early_stop_mode\": early_stop_mode,\n",
    "        \"early_stop_patience\": early_stop_patience,\n",
    "        \"export_dir\": export_dir\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "print(\"Saved extra config to:\", extra_cfg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba603d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Flow Precompute & Caching Utilities\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "def _safe_relpath(video_path: str, root: str) -> str:\n",
    "    try:\n",
    "        rp = os.path.relpath(video_path, root)\n",
    "    except Exception:\n",
    "        rp = os.path.basename(video_path)\n",
    "    return rp.replace(\"\\\\\", \"/\")\n",
    "\n",
    "def _cache_path_for_video(video_path: str, cache_root: str) -> str:\n",
    "    # deterministic path inside cache_root mirroring class/subdir structure, with .npy filename\n",
    "    base = Path(video_path)\n",
    "    # hash full path to avoid collisions\n",
    "    h = hashlib.sha1(video_path.encode(\"utf-8\")).hexdigest()[:10]\n",
    "    rel = _safe_relpath(video_path, cfg.data_root)\n",
    "    rel_no_ext = os.path.splitext(rel)[0]\n",
    "    dst_dir = Path(cache_root) / Path(rel_no_ext).parent\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return str(dst_dir / f\"{Path(rel_no_ext).name}__{h}.npy\")\n",
    "\n",
    "def compute_pairwise_flows(frames: list, method: str=\"tvl1\"):\n",
    "    flows = []\n",
    "    if method == \"tvl1\":\n",
    "        tvl1 = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "        for t in range(len(frames)-1):\n",
    "            prev = cv2.cvtColor(frames[t], cv2.COLOR_RGB2GRAY)\n",
    "            nxt  = cv2.cvtColor(frames[t+1], cv2.COLOR_RGB2GRAY)\n",
    "            flow = tvl1.calc(prev, nxt, None).astype(np.float32)  # HxWx2\n",
    "            flows.append(flow)\n",
    "    else:\n",
    "        for t in range(len(frames)-1):\n",
    "            prev = cv2.cvtColor(frames[t], cv2.COLOR_RGB2GRAY)\n",
    "            nxt  = cv2.cvtColor(frames[t+1], cv2.COLOR_RGB2GRAY)\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev, nxt, None,\n",
    "                                                pyr_scale=0.5, levels=3, winsize=15,\n",
    "                                                iterations=3, poly_n=5, poly_sigma=1.1, flags=0).astype(np.float32)\n",
    "            flows.append(flow)\n",
    "    return flows  # list of (H,W,2)\n",
    "\n",
    "def precompute_flow_for_video(video_path: str, out_path: str, frame_size: int, method: str):\n",
    "    # Read frames, resize+crop exactly like training\n",
    "    frames_all = read_all_frames_cv(video_path)\n",
    "    frames_all = [resize_and_center_crop(f, frame_size) for f in frames_all]\n",
    "    if len(frames_all) < 2:\n",
    "        np.save(out_path, np.zeros((0, frame_size, frame_size, 2), dtype=np.float32))\n",
    "        return out_path\n",
    "    flows = compute_pairwise_flows(frames_all, method=method)\n",
    "    flows = np.stack(flows, axis=0)  # (N-1, H, W, 2)\n",
    "    # Robust scale per-video (clip 95th percentile then scale to [-1,1] approx)\n",
    "    mag = np.sqrt(np.sum(flows**2, axis=3, keepdims=True))\n",
    "    m95 = np.percentile(mag, 95) + 1e-6\n",
    "    flows = np.clip(flows, -m95, m95) / m95\n",
    "    np.save(out_path, flows.astype(np.float32))\n",
    "    return out_path\n",
    "\n",
    "def precompute_flow_for_split(samples, cache_root: str, frame_size: int, method: str=\"tvl1\"):\n",
    "    paths = []\n",
    "    for i, (vp, _) in enumerate(samples, 1):\n",
    "        dst = _cache_path_for_video(vp, cache_root)\n",
    "        if not os.path.exists(dst):\n",
    "            try:\n",
    "                precompute_flow_for_video(vp, dst, frame_size, method)\n",
    "                print(f\"[{i}/{len(samples)}] cached flow:\", dst)\n",
    "            except Exception as e:\n",
    "                print(f\"[{i}/{len(samples)}] FAILED {vp}: {e}\")\n",
    "        else:\n",
    "            # print(f\"[{i}/{len(samples)}] exists:\", dst)\n",
    "            pass\n",
    "        paths.append((vp, dst))\n",
    "    # Save an index mapping\n",
    "    index_path = os.path.join(cache_root, \"index.json\")\n",
    "    os.makedirs(cache_root, exist_ok=True)\n",
    "    # read existing index (merge)\n",
    "    idx = {}\n",
    "    if os.path.exists(index_path):\n",
    "        try:\n",
    "            with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                idx = json.load(f)\n",
    "        except Exception:\n",
    "            idx = {}\n",
    "    for vp, dst in paths:\n",
    "        idx[vp] = dst\n",
    "    with open(index_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(idx, f, indent=2, ensure_ascii=False)\n",
    "    print(\"Updated flow cache index:\", index_path)\n",
    "    return index_path\n",
    "\n",
    "# Optional: run precompute now\n",
    "os.makedirs(cfg.flow_cache_dir, exist_ok=True)\n",
    "if cfg.precompute_flow_now:\n",
    "    target = []\n",
    "    if cfg.precompute_on_split in (\"train\", \"train+val\", \"all\"):\n",
    "        target += splits[\"train\"]\n",
    "    if cfg.precompute_on_split in (\"val\", \"train+val\", \"all\"):\n",
    "        target += splits[\"val\"]\n",
    "    precompute_flow_for_split(target, cfg.flow_cache_dir, cfg.frame_size, method=cfg.cache_flow_method)\n",
    "else:\n",
    "    print(\"Skip flow precompute (set precompute_flow_now=True to enable). Cache dir:\", cfg.flow_cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfb67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Dataset (v2) using Cached Flows if enabled\n",
    "\n",
    "# Helper to form flow stack from cached pairwise flows around a center index\n",
    "def flow_stack_from_cache(flow_pairs: np.ndarray, center_index: int, stack: int) -> np.ndarray:\n",
    "    # flow_pairs shape: (N-1, H, W, 2), where pair t is flow from frame t -> t+1\n",
    "    N_1 = flow_pairs.shape[0]\n",
    "    pairs = []  # we want exactly 'stack' pairs ending near center\n",
    "    # preferentially take backward pairs ending at center: (center-1)->center, ... (center-stack)->(center-stack+1)\n",
    "    for k in range(stack):\n",
    "        t = center_index - 1 - k\n",
    "        if 0 <= t < N_1:\n",
    "            pairs.append(t)\n",
    "    pairs = pairs[::-1]  # ascending\n",
    "    # if not enough, add forward pairs starting at center\n",
    "    t = center_index\n",
    "    while len(pairs) < stack and t < N_1:\n",
    "        pairs.append(t)\n",
    "        t += 1\n",
    "    if len(pairs) == 0:\n",
    "        # degenerate case: no pairs available\n",
    "        H, W = flow_pairs.shape[1:3]\n",
    "        return np.zeros((H, W, 2*stack), dtype=np.float32)\n",
    "    # Gather and concat along channel\n",
    "    flows = flow_pairs[pairs]  # (S, H, W, 2)\n",
    "    uv_stack = np.concatenate([f for f in flows], axis=2)  # (H, W, 2S)\n",
    "    return uv_stack.astype(np.float32)\n",
    "\n",
    "class TwoStreamVideoDataset(Dataset):\n",
    "    def __init__(self, samples: List[Tuple[str,int]], class_names: List[str], cfg):\n",
    "        self.samples = samples\n",
    "        self.class_names = class_names\n",
    "        self.cfg = cfg\n",
    "        self.rgb_tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.flow_tf = transforms.Compose([ transforms.ToTensor() ])\n",
    "\n",
    "        # load cache index if any\n",
    "        self.flow_cache_index = {}\n",
    "        if getattr(cfg, \"use_flow_cache\", False):\n",
    "            index_path = os.path.join(cfg.flow_cache_dir, \"index.json\")\n",
    "            if os.path.exists(index_path):\n",
    "                with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    self.flow_cache_index = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.samples[idx]\n",
    "        frames_all = read_all_frames_cv(video_path)\n",
    "        frames_all = [resize_and_center_crop(f, self.cfg.frame_size) for f in frames_all]\n",
    "        n = len(frames_all)\n",
    "\n",
    "        seg_inds_rgb = sample_indices(n, self.cfg.num_segments, self.cfg.clip_len_rgb)\n",
    "        rgb_indices = [inds[len(inds)//2] for inds in seg_inds_rgb]\n",
    "        flow_centers = rgb_indices\n",
    "\n",
    "        # RGB clip\n",
    "        rgb_imgs = [frames_all[i] for i in rgb_indices]\n",
    "        rgb_tensors = [self.rgb_tf(img) for img in rgb_imgs]\n",
    "        rgb_clip = torch.stack(rgb_tensors, dim=0)  # Kx3xHxW\n",
    "\n",
    "        # Flow clip\n",
    "        flow_stacks = []\n",
    "        if getattr(self.cfg, \"use_flow_cache\", False):\n",
    "            # load cached pairwise flows for this video if exists; otherwise fall back\n",
    "            flow_cache_path = self.flow_cache_index.get(video_path, _cache_path_for_video(video_path, self.cfg.flow_cache_dir))\n",
    "            if not os.path.exists(flow_cache_path):\n",
    "                # compute on the fly once and cache\n",
    "                try:\n",
    "                    precompute_flow_for_video(video_path, flow_cache_path, self.cfg.frame_size, getattr(self.cfg, \"cache_flow_method\", self.cfg.flow_method))\n",
    "                except Exception as e:\n",
    "                    # fallback: compute on the fly per stack\n",
    "                    flow_cache_path = None\n",
    "            if flow_cache_path and os.path.exists(flow_cache_path):\n",
    "                pairwise_flows = np.load(flow_cache_path)  # (N-1,H,W,2) scaled\n",
    "                for c in flow_centers:\n",
    "                    uv = flow_stack_from_cache(pairwise_flows, c, stack=self.cfg.flow_stack)\n",
    "                    flow_stacks.append(self.flow_tf(uv))\n",
    "            else:\n",
    "                for c in flow_centers:\n",
    "                    uv = compute_flow_stack(frames_all, c, stack=self.cfg.flow_stack, method=self.cfg.flow_method)\n",
    "                    flow_stacks.append(self.flow_tf(uv))\n",
    "        else:\n",
    "            for c in flow_centers:\n",
    "                uv = compute_flow_stack(frames_all, c, stack=self.cfg.flow_stack, method=self.cfg.flow_method)\n",
    "                flow_stacks.append(self.flow_tf(uv))\n",
    "\n",
    "        flow_clip = torch.stack(flow_stacks, dim=0)  # Kx(2*stack)xHxW\n",
    "\n",
    "        return rgb_clip, flow_clip, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Train (with LR scheduler + Early Stopping)\n",
    "\n",
    "def make_scheduler(optimizer):\n",
    "    if cfg.lr_scheduler_type == \"step\":\n",
    "        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=cfg.step_size, gamma=cfg.gamma)\n",
    "    elif cfg.lr_scheduler_type == \"plateau\":\n",
    "        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\" if cfg.early_stop_on==\"val_acc\" else \"min\",\n",
    "                                                          factor=cfg.gamma, patience=max(1, cfg.early_stop_patience//2), verbose=True)\n",
    "    elif cfg.lr_scheduler_type == \"cosine\":\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.cosine_Tmax)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def is_better(a, b, mode):\n",
    "    if b is None: return True\n",
    "    return (a > b) if mode == \"max\" else (a < b)\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "metrics_log = []\n",
    "\n",
    "best_metric = None\n",
    "best_epoch = 0\n",
    "no_improve = 0\n",
    "\n",
    "scheduler = make_scheduler(optimizer)\n",
    "\n",
    "for epoch in range(1, cfg.epochs+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, epoch, cfg)\n",
    "    va_loss, va_acc, y_true, y_pred = evaluate(model, val_loader)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    ep_metrics = {\"epoch\": epoch, \"train_loss\": tr_loss, \"train_acc\": tr_acc, \"val_loss\": va_loss, \"val_acc\": va_acc}\n",
    "    metrics_log.append(ep_metrics)\n",
    "\n",
    "    # choose monitored metric\n",
    "    current = va_acc if cfg.early_stop_on == \"val_acc\" else va_loss\n",
    "    mode = \"max\" if (cfg.early_stop_on == \"val_acc\") else \"min\"\n",
    "\n",
    "    # step scheduler\n",
    "    if scheduler is not None:\n",
    "        if cfg.lr_scheduler_type == \"plateau\":\n",
    "            scheduler.step(current)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "    # checkpoint\n",
    "    print(f\"Epoch {epoch:02d}: train_loss={tr_loss:.4f}, train_acc={tr_acc:.4f} | val_loss={va_loss:.4f}, val_acc={va_acc:.4f}\")\n",
    "    save_ckpt(model, optimizer, epoch, cfg, class_names)\n",
    "\n",
    "    # track best & early stopping\n",
    "    if is_better(current, best_metric, mode):\n",
    "        best_metric = current\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"cfg\": asdict(cfg),\n",
    "            \"class_names\": class_names\n",
    "        }, os.path.join(cfg.output_dir, \"best_model.pth\"))\n",
    "        print(\"Saved best model (epoch\", epoch, \")\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"No improvement for {no_improve}/{cfg.early_stop_patience} epochs (best @ epoch {best_epoch}).\")\n",
    "        if no_improve >= cfg.early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save history & metrics\n",
    "with open(os.path.join(cfg.output_dir, \"history.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "with open(os.path.join(cfg.output_dir, \"metrics_log.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_log, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Training finished. Best epoch:\", best_epoch, \"| Best metric:\", best_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Export: state_dict (.pt) and TorchScript (.pt)\n",
    "\n",
    "# Save state_dict (weights only)\n",
    "state_dict_path = os.path.join(cfg.export_dir, \"two_stream_state_dict.pt\")\n",
    "torch.save(model.state_dict(), state_dict_path)\n",
    "print(\"Saved state_dict to:\", state_dict_path)\n",
    "\n",
    "# TorchScript (script) export\n",
    "try:\n",
    "    scripted = torch.jit.script(model.eval())\n",
    "    ts_path = os.path.join(cfg.export_dir, \"two_stream_scripted.pt\")\n",
    "    scripted.save(ts_path)\n",
    "    print(\"Saved TorchScript scripted model to:\", ts_path)\n",
    "except Exception as e:\n",
    "    print(\"TorchScript scripting failed, falling back to tracing. Reason:\", e)\n",
    "    # Create dummy inputs for tracing\n",
    "    K = cfg.num_segments\n",
    "    C_rgb = 3\n",
    "    C_flow = 2*cfg.flow_stack\n",
    "    H = W = cfg.frame_size\n",
    "    dummy_rgb = torch.randn(1, K, C_rgb, H, W, device=device)\n",
    "    dummy_flow = torch.randn(1, K, C_flow, H, W, device=device)\n",
    "    traced = torch.jit.trace(model.eval(), (dummy_rgb, dummy_flow))\n",
    "    ts_path = os.path.join(cfg.export_dir, \"two_stream_traced.pt\")\n",
    "    traced.save(ts_path)\n",
    "    print(\"Saved TorchScript traced model to:\", ts_path)\n",
    "\n",
    "print(\"Export artifacts are in:\", cfg.export_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
